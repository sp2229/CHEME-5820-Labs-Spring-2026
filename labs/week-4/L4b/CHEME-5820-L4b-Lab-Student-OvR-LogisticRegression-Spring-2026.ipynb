{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a39ab0",
   "metadata": {},
   "source": [
    "# L4b: One versus the Rest Strategy for Multi-class Classification\n",
    "In this lab, we implement the One versus the Rest (OvR) strategy, which extends binary logistic regression to multi-class problems by training one classifier per class. Each classifier learns to distinguish its assigned class from all others. OvR is computationally efficient and interpretable: rather than learning a complex multinomial model, we leverage multiple simple binary classifiers.\n",
    "\n",
    "> __Learning Objectives:__\n",
    "> \n",
    "> By the end of this lab, you will be able to:\n",
    "> \n",
    "> * __Motivate OvR as a practical multiclass strategy:__ Explain how a multiclass problem becomes a collection of binary decisions and why that makes training and interpretation simpler.\n",
    "> * __Shape the data to match the model:__ Build balanced per-class datasets from MNIST and justify a feature-reduction step to keep optimization tractable.\n",
    "> * __Connect evaluation to model behavior:__ Use a digit-vs-rest confusion matrix to diagnose false positives and false negatives, and relate them to accuracy, precision, and recall.\n",
    "\n",
    "Let's get started!\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8f6a02",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "First, we set up the computational environment by including the `Include.jl` file and loading any needed resources.\n",
    "\n",
    "> The [`include(...)` command](https://docs.julialang.org/en/v1/base/base/#include) evaluates the contents of the input source file, `Include.jl`, in the notebook's global scope. The `Include.jl` file sets paths, loads required external packages, etc. For additional information on functions and types used in this material, see the [Julia programming language documentation](https://docs.julialang.org/en/v1/). \n",
    "\n",
    "Let's set up our code environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85f06385",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(joinpath(@__DIR__, \"Include.jl\")); # include the Include.jl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbf98ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#= \n",
    "imagine n categories and find out if u are in category 1 or another category (not one)\n",
    "one versus the rest (binary decision) made w logistic regression\n",
    "=#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a78e46",
   "metadata": {},
   "source": [
    "In addition to standard Julia libraries, we'll also use [the `VLDataScienceMachineLearningPackage.jl` package](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl). Check out [the documentation](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/) for more information on the functions, types, and data used in this material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0e7ad2",
   "metadata": {},
   "source": [
    "### Constants\n",
    "We define constants that control data loading, image dimensions, and the train/test split. The `number_of_examples::Int` constant sets how many images per digit to load from MNIST, while `number_of_test_examples::Int` reserves a portion for evaluation. The remaining constants specify the image geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73bb461e",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_examples = 3000; # how many training examples of *each* number to include from the library\n",
    "number_of_test_examples = 500; # how many examples are we going to test on?\n",
    "number_of_training_examples = number_of_examples - number_of_test_examples; # how many training examples of *each* number to include from the library\n",
    "number_digit_array = range(0,length=10,step=1) |> collect; # numbers 0 ... 9\n",
    "number_of_rows = 28; # number of rows in the image\n",
    "number_of_cols = 28; # number of cols in the image\n",
    "number_of_pixels = number_of_rows*number_of_cols; # how many pixels do we have in the image?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78e3a98",
   "metadata": {},
   "source": [
    "### Data\n",
    "Our data consists of images of handwritten digits (0–9) from the [Modified National Institute of Standards and Technology (MNIST) database](https://en.wikipedia.org/wiki/MNIST_database). \n",
    "\n",
    "We build a training dataset to estimate model parameters, stored in the `training_image_dataset::Vector{Tuple{Vector{Float64}, OneHotVector{UInt32}}}` variable, and a test dataset to evaluate generalization to unseen data, stored in the `testing_image_dataset::Vector{Tuple{Vector{Float64}, OneHotVector{UInt32}}}` variable.\n",
    "\n",
    "> __Data format:__ The first element of each tuple is the input data $\\mathbf{x}$ (the image pixels arranged as a vector), and the second element is the label (whether the image corresponds to digits 0–9).\n",
    ">\n",
    "> __Type considerations:__ The input data uses `Float64` precision for numerical stability in gradient-based optimization. The labels are [one-hot encoded](https://en.wikipedia.org/wiki/One-hot), and the input data is stored as a vector rather than a matrix (even though the original image is a $28\\times 28$ matrix of grayscale values).\n",
    "\n",
    "We load `number_of_examples::Int` images per digit into the `digits_image_dictionary::Dict{Int, Array{N0f8,3}}` dictionary, where each key is a digit (0–9) and each value is a 3D array of grayscale images. We then convert these to vector format by linearizing the $28\\times 28$ matrix of grayscale values into a vector of 784 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7691eaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_image_dictionary = MyMNISTHandwrittenDigitImageDataset(number_of_examples = number_of_examples);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9609940c",
   "metadata": {},
   "source": [
    "Let's inspect the `digits_image_dictionary::Dict{Int, Array{N0f8,3}}` variable. Each entry maps a digit to a $28\\times 28\\times n$ array, where $n$ is the number of images for that digit. We can index into the array to view individual images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7df7f1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAABC9JREFUaAW9wV+oZHUdAPDPOec7c//srnI3a30LMiGicPFBkkhJqAgkhR6MpB7yIYgiFEkIgghBSAgDEyJIVnqpSB/yZS2FfMkHUYgkSRDBEHRdbu7de2fmzJxzGvgJs8Nc2rfv5xOuUGEHM3SKBh0COzhQVBhQK3pFoMLc8UKykCwkC0uhWOBIMUaDiWKBA1QYo1X0ii3sYl8xQmBiXUgWkoVkUWFh0xytYg8tDhFoMSh2cYQZesU25pjYFJKFZCFZjNAqTmCKDoOVfTSKAQNqDDjCGCMcKqYI7OLIupAsJAvJolU0aNEpapxEg31UioXiGhxirugU1+EiFljYFJKFZCFZWNrCDB0ajDDFJcUn8RBux42YYAeHOIE38AQew/v+v5AsJAvJwlJl5RQmio/ip/g8bsIFRa04gRluxEOo8WscokaDuXUhWUgWkoUr1JhihtP4Eb6B09jHeXwNHf6DFm/hNpzBF/BLRaC1KSQLyUKy2MFEsY0jxa14EC3+ggfwGs7iAG9hgcCDeAR3INChdryQLCQLyWJiZaa4BrdgjjG+rNjFq1YCC0wU/0armKJBZ11IFpKFZOEKveIyrkNgodjDvmIP+xihwk3ocIDBSmdTSBaShWTRoFOM0aLH66gQuBmvoEaPfYwwwfX4KhaYo8aAEVqbQrKQLCSLDjV6DBhjhr/hTXwCL+P7eBItBsxxCp/D9RjwEnpsoUeN3rqQLCQLycLSFiZo0WAb/8DP8ShO4WHcj7/jPTyLf+E+9DjA04oKc4zRWheShWQhWVgaUGFAh05xDiM8jD3sYQ8fwf2oMcMCv8Nril7R2hSShWQhWViaIlCjQ4czeBe/wpOY4xF8D5dxElOMMcOj6K006GwKyUKykCwsVeiwsHIBI8zRosM5fAdjDHgbH8O1+CN+hr9iihqBhXUhWUgWkoWlATUq9IoevWKOs3hV8QbO4ye4HX/AzTiHb+I8RpjZFJKFZCFZ+FClqDBYOY278Bu0eAH34R3F8/gSHsdn8RQ+jf86XkgWkoVk4UODYrDuTvwWF/FPfBfvKBpcxov4Mf6MLTyBe3AKB9aFZCFZSBaVYsCgqBBo8EPFu/g6LmILMzQ4gUt4Fk/hXnwc2ziwKSQLyUKyqDBgsFKhQeAGxeO4iJM4UrRoEdjDaTQ4g6njhWQhWUgWg6LCoBjQocLrOIvHcCt+gUt4EzvYw934AT6FQ7yCGr1NIVlIFpLFgAqVYsCABXrciefwGdyNe1HjRWzjFnyAk2jxNr6FbRzZFJKFZCFZWBowWDegw/v4Cu7At/FF7OA2DIoGL+BPeA5T9I4XkoVkIVm4im1cwO/xDGpMrQs0mFkJVJhbF5KFZCFZuIq5ldbKGLv4AA0q1OhRYeF4IVlIFpKFq+iwhRE6tOjQolXMrNtCjQ4z60KykCwk+x+gryLGGYa8IwAAAABJRU5ErkJggg==",
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAABC9JREFUaAW9wV+oZHUdAPDPOec7c//srnI3a30LMiGicPFBkkhJqAgkhR6MpB7yIYgiFEkIgghBSAgDEyJIVnqpSB/yZS2FfMkHUYgkSRDBEHRdbu7de2fmzJxzGvgJs8Nc2rfv5xOuUGEHM3SKBh0COzhQVBhQK3pFoMLc8UKykCwkC0uhWOBIMUaDiWKBA1QYo1X0ii3sYl8xQmBiXUgWkoVkUWFh0xytYg8tDhFoMSh2cYQZesU25pjYFJKFZCFZjNAqTmCKDoOVfTSKAQNqDDjCGCMcKqYI7OLIupAsJAvJolU0aNEpapxEg31UioXiGhxirugU1+EiFljYFJKFZCFZWNrCDB0ajDDFJcUn8RBux42YYAeHOIE38AQew/v+v5AsJAvJwlJl5RQmio/ip/g8bsIFRa04gRluxEOo8WscokaDuXUhWUgWkoUr1JhihtP4Eb6B09jHeXwNHf6DFm/hNpzBF/BLRaC1KSQLyUKy2MFEsY0jxa14EC3+ggfwGs7iAG9hgcCDeAR3INChdryQLCQLyWJiZaa4BrdgjjG+rNjFq1YCC0wU/0armKJBZ11IFpKFZOEKveIyrkNgodjDvmIP+xihwk3ocIDBSmdTSBaShWTRoFOM0aLH66gQuBmvoEaPfYwwwfX4KhaYo8aAEVqbQrKQLCSLDjV6DBhjhr/hTXwCL+P7eBItBsxxCp/D9RjwEnpsoUeN3rqQLCQLycLSFiZo0WAb/8DP8ShO4WHcj7/jPTyLf+E+9DjA04oKc4zRWheShWQhWVgaUGFAh05xDiM8jD3sYQ8fwf2oMcMCv8Nril7R2hSShWQhWViaIlCjQ4czeBe/wpOY4xF8D5dxElOMMcOj6K006GwKyUKykCwsVeiwsHIBI8zRosM5fAdjDHgbH8O1+CN+hr9iihqBhXUhWUgWkoWlATUq9IoevWKOs3hV8QbO4ye4HX/AzTiHb+I8RpjZFJKFZCFZ+FClqDBYOY278Bu0eAH34R3F8/gSHsdn8RQ+jf86XkgWkoVk4UODYrDuTvwWF/FPfBfvKBpcxov4Mf6MLTyBe3AKB9aFZCFZSBaVYsCgqBBo8EPFu/g6LmILMzQ4gUt4Fk/hXnwc2ziwKSQLyUKyqDBgsFKhQeAGxeO4iJM4UrRoEdjDaTQ4g6njhWQhWUgWg6LCoBjQocLrOIvHcCt+gUt4EzvYw934AT6FQ7yCGr1NIVlIFpLFgAqVYsCABXrciefwGdyNe1HjRWzjFnyAk2jxNr6FbRzZFJKFZCFZWBowWDegw/v4Cu7At/FF7OA2DIoGL+BPeA5T9I4XkoVkIVm4im1cwO/xDGpMrQs0mFkJVJhbF5KFZCFZuIq5ldbKGLv4AA0q1OhRYeF4IVlIFpKFq+iwhRE6tOjQolXMrNtCjQ4z60KykCwk+x+gryLGGYa8IwAAAABJRU5ErkJg\">"
      ],
      "text/plain": [
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;235m▀\u001b[38;5;232;48;5;247m▀\u001b[38;5;232;48;5;253m▀\u001b[38;5;232;48;5;255m▀\u001b[38;5;232;48;5;255m▀\u001b[38;5;233;48;5;255m▀\u001b[38;5;232;48;5;255m▀\u001b[38;5;232;48;5;255m▀\u001b[38;5;232;48;5;253m▀\u001b[38;5;232;48;5;242m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;234;48;5;245m▀\u001b[38;5;246;48;5;255m▀\u001b[38;5;252;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;254;48;5;247m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;253m▀\u001b[38;5;255;48;5;252m▀\u001b[38;5;255;48;5;249m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;254;48;5;255m▀\u001b[38;5;243;48;5;249m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;239;48;5;244m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;250;48;5;232m▀\u001b[38;5;236;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;236;48;5;232m▀\u001b[38;5;235;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;243;48;5;242m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;251m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;246;48;5;242m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;232;48;5;236m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;234m▀\u001b[38;5;247;48;5;254m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;250;48;5;250m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;236;48;5;232m▀\u001b[38;5;253;48;5;242m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;242;48;5;255m▀\u001b[38;5;232;48;5;253m▀\u001b[38;5;232;48;5;247m▀\u001b[38;5;232;48;5;245m▀\u001b[38;5;232;48;5;238m▀\u001b[38;5;233;48;5;234m▀\u001b[38;5;239;48;5;249m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;244;48;5;236m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;234m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;242;48;5;232m▀\u001b[38;5;255;48;5;233m▀\u001b[38;5;255;48;5;232m▀\u001b[38;5;255;48;5;245m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;246;48;5;235m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;236m▀\u001b[38;5;236;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;253m▀\u001b[38;5;255;48;5;246m▀\u001b[38;5;253;48;5;233m▀\u001b[38;5;255;48;5;239m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;251;48;5;255m▀\u001b[38;5;239;48;5;255m▀\u001b[38;5;232;48;5;238m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;241m▀\u001b[38;5;241;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;250m▀\u001b[38;5;249;48;5;234m▀\u001b[38;5;234;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;238;48;5;232m▀\u001b[38;5;252;48;5;239m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;254;48;5;255m▀\u001b[38;5;233;48;5;248m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;252;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;252;48;5;242m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;234m▀\u001b[38;5;247;48;5;249m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;251;48;5;253m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;251;48;5;233m▀\u001b[38;5;255;48;5;252m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;247;48;5;255m▀\u001b[38;5;236;48;5;255m▀\u001b[38;5;235;48;5;255m▀\u001b[38;5;232;48;5;250m▀\u001b[38;5;232;48;5;251m▀\u001b[38;5;234;48;5;255m▀\u001b[38;5;242;48;5;255m▀\u001b[38;5;253;48;5;255m▀\u001b[38;5;255;48;5;255m▀\u001b[38;5;255;48;5;253m▀\u001b[38;5;248;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;234;48;5;232m▀\u001b[38;5;240;48;5;232m▀\u001b[38;5;249;48;5;232m▀\u001b[38;5;255;48;5;232m▀\u001b[38;5;255;48;5;232m▀\u001b[38;5;255;48;5;232m▀\u001b[38;5;255;48;5;232m▀\u001b[38;5;255;48;5;232m▀\u001b[38;5;255;48;5;232m▀\u001b[38;5;250;48;5;232m▀\u001b[38;5;242;48;5;232m▀\u001b[38;5;235;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;233m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;233;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[38;5;232;48;5;232m▀\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "digits_image_dictionary[8][ :, :, 10] # how does the indexing work? This is the 10th example of the digit \"8\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a8b946",
   "metadata": {},
   "source": [
    "Next, let's partition the `digits_image_dictionary::Dict{Int, Array{N0f8,3}}` into training and testing datasets. We randomly select `number_of_training_examples::Int` images per digit for training, and the remaining images are used for testing. In each case, we convert the $28\\times 28$ images into vector format by linearizing the matrix into a vector of 784 pixels.\n",
    "\n",
    "Let's start with the training dataset. \n",
    "\n",
    "> __What is vectorization?__ Each $N\\times N$ image array containing grayscale values at each pixel is converted to an $N^{2}$ vector by concatenating pixel values. The image class (the digit it represents) is converted to [one-hot format](https://en.wikipedia.org/wiki/One-hot). \n",
    "\n",
    "Let's save the training data in the `training_image_dataset::Vector{Tuple{Vector{Float64}, OneHotVector{UInt32}}}` variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43cd09f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000-element Vector{Tuple{Vector{Float64}, OneHotVector{UInt32}}}:\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03137254901960784, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00784313725490196, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03529411764705882  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00784313725490196  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01568627450980392  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023529411764705882  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.050980392156862744  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023529411764705882, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       " ⋮\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01568627450980392, 0.011764705882352941  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03529411764705882  …  0.01568627450980392, 0.00784313725490196, 0.011764705882352941, 0.0, 0.0, 0.0196078431372549, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.023529411764705882, 0.0, 0.0, 0.023529411764705882, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011764705882352941, 0.00784313725490196  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023529411764705882, 0.00392156862745098  …  0.0, 0.0, 0.07450980392156863, 0.050980392156862744, 0.0, 0.011764705882352941, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00784313725490196  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027450980392156862, 0.00392156862745098  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
       " ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.043137254901960784, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_image_dataset = let\n",
    "\n",
    "    # initialize - empty collection to hold (image vector, one-hot label) tuples\n",
    "    training_image_dataset = Vector{Tuple{Vector{Float64}, OneHotVector{UInt32}}}();\n",
    "\n",
    "    # build training tuples - iterate over each digit class (0, 1, ..., 9)\n",
    "    for i ∈ number_digit_array\n",
    "        Y = onehot(i, number_digit_array); # encode digit i as a one-hot vector over 0–9\n",
    "        X = digits_image_dictionary[i]; # grab the 28×28×n array of all images for digit i\n",
    "\n",
    "        for t ∈ 1:number_of_training_examples\n",
    "            D = reshape(transpose(X[:,:,t]) |> Matrix, number_of_pixels) |> vec; # flatten the 28×28 image matrix into a 784-element vector\n",
    "            push!(training_image_dataset, (D, Y)); # store the (image vector, label) pair\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # return -\n",
    "    training_image_dataset\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c497eaf9",
   "metadata": {},
   "source": [
    "Next, we load `number_of_test_examples::Int` images from the `digits_image_dictionary::Dict{Int, Array{N0f8,3}}` for testing purposes. We save the test data in the `testing_image_dataset::Vector{Tuple{Vector{Float64}, OneHotVector{UInt32}}}` variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b05c5640",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_image_dataset = let\n",
    "    \n",
    "    # initialize - empty collection to hold (image vector, one-hot label) tuples\n",
    "    testing_image_dataset = Vector{Tuple{Vector{Float64}, OneHotVector{UInt32}}}()\n",
    "    \n",
    "    # build testing tuples - use images *after* the training partition for each digit\n",
    "    for i ∈ number_digit_array\n",
    "        Y = onehot(i, number_digit_array); # encode digit i as a one-hot vector over 0–9\n",
    "        X = digits_image_dictionary[i]; # grab the 28×28×n array of all images for digit i\n",
    "        \n",
    "        for t ∈ (number_of_training_examples+1):number_of_examples\n",
    "            D = reshape(transpose(X[:,:,t]) |> Matrix, number_of_pixels) |> vec; # flatten the 28×28 image matrix into a 784-element vector\n",
    "            push!(testing_image_dataset, (D, Y)); # store the (image vector, label) pair\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # return -\n",
    "    testing_image_dataset\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d3de69",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Before constructing the OvR datasets, we define three helper functions that will be used across all tasks:\n",
    "\n",
    "> __Helper functions:__\n",
    ">\n",
    "> * `downsample_image(x::Vector{Float64}, pool::Int)::Vector{Float64}` reduces a 784-pixel image vector to a smaller feature vector by average pooling over non-overlapping $\\text{pool}\\times\\text{pool}$ blocks.\n",
    "> * `build_ovr_dataset(...)` takes the full image dataset and a target digit, then constructs a balanced binary dataset with $\\pm 1$ labels and downsampled features.\n",
    "> * `ovr_confusion_for_digit(digit::Int; ...)` evaluates a trained OvR classifier on test data and returns the confusion matrix, accuracy, predictions, true labels, and class probabilities.\n",
    "\n",
    "Let's define these functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9025d0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    downsample_image(x::Vector{Float64}, pool::Int)::Vector{Float64}\n",
    "\n",
    "Reduce a 784-pixel image vector to a smaller feature vector by average pooling \n",
    "over non-overlapping pool×pool blocks. For pool = 4, a 28×28 image becomes 7×7 = 49 features.\n",
    "\"\"\"\n",
    "function downsample_image(x::Vector{Float64}, pool::Int)::Vector{Float64}\n",
    "\n",
    "    # reshape - convert the 784-element vector back into a 28×28 image matrix\n",
    "    # note: we transpose because the original vectorization used column-major order\n",
    "    img = reshape(x, (number_of_cols, number_of_rows))';\n",
    "    new_rows = number_of_rows ÷ pool; # number of rows in the downsampled image (28÷4 = 7)\n",
    "    new_cols = number_of_cols ÷ pool; # number of cols in the downsampled image (28÷4 = 7)\n",
    "\n",
    "    # average pooling - replace each pool×pool block with its mean value\n",
    "    pooled = zeros(Float64, new_rows, new_cols);\n",
    "    for i ∈ 1:new_rows\n",
    "        for j ∈ 1:new_cols\n",
    "            r = (pool*(i-1)+1):(pool*i); # row range for this block\n",
    "            c = (pool*(j-1)+1):(pool*j); # col range for this block\n",
    "            pooled[i,j] = mean(img[r,c]); # average all pixel values in this block\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # return - flatten the pooled matrix back into a vector\n",
    "    return vec(pooled);\n",
    "end;\n",
    "\n",
    "\"\"\"\n",
    "    build_ovr_dataset(image_dataset, digit; number_positive, number_negative, pool)\n",
    "\n",
    "Build a balanced binary dataset for one-vs-rest classification. \n",
    "Returns (X, y) where X is the downsampled feature matrix and y ∈ {+1, -1}.\n",
    "\"\"\"\n",
    "function build_ovr_dataset(image_dataset::Vector{Tuple{Vector{Float64}, OneHotVector{UInt32}}},\n",
    "    digit::Int; number_positive::Int = 200, number_negative::Int = 200, pool::Int = 4)\n",
    "\n",
    "    # collect indices for positive and negative classes -\n",
    "    # positive = images that ARE the target digit, negative = everything else\n",
    "    pos_idx = Int[];\n",
    "    neg_idx = Int[];\n",
    "    for (i, (x, y_onehot)) ∈ enumerate(image_dataset)\n",
    "        label = number_digit_array[argmax(y_onehot)]; # decode one-hot label to integer digit\n",
    "        if (label == digit)\n",
    "            push!(pos_idx, i); # this image matches the target digit\n",
    "        else\n",
    "            push!(neg_idx, i); # this image is some other digit\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # sample a balanced subset - randomly pick equal numbers of positive and negative examples\n",
    "    pos_sel = pos_idx[randperm(length(pos_idx))[1:number_positive]]; # random sample of positive indices\n",
    "    neg_sel = neg_idx[randperm(length(neg_idx))[1:number_negative]]; # random sample of negative indices\n",
    "    selected = vcat(pos_sel, neg_sel); # combine positive and negative indices\n",
    "    selected = selected[randperm(length(selected))]; # shuffle so positives and negatives are interleaved\n",
    "\n",
    "    # allocate feature matrix and label vector -\n",
    "    features_per_image = (number_of_rows ÷ pool) * (number_of_cols ÷ pool); # e.g., 7×7 = 49 for pool = 4\n",
    "    X = Array{Float64,2}(undef, length(selected), features_per_image); # each row is one downsampled image\n",
    "    y = Array{Int64,1}(undef, length(selected)); # binary label for each example\n",
    "    for (k, idx) ∈ enumerate(selected)\n",
    "        x, y_onehot = image_dataset[idx]; # grab the image vector and one-hot label\n",
    "        X[k,:] = downsample_image(x, pool); # downsample and store as row k of the feature matrix\n",
    "        label = number_digit_array[argmax(y_onehot)]; # decode one-hot to integer digit\n",
    "        y[k] = (label == digit) ? 1 : -1; # +1 if this is the target digit, -1 otherwise\n",
    "    end\n",
    "\n",
    "    # return - feature matrix X and binary label vector y\n",
    "    return X, y;\n",
    "end;\n",
    "\n",
    "\"\"\"\n",
    "    ovr_confusion_for_digit(digit; dataset, models)\n",
    "\n",
    "Evaluate the trained OvR classifier for a single digit on its test dataset.\n",
    "Returns the confusion matrix, accuracy, predicted labels, true labels, and probability matrix.\n",
    "\"\"\"\n",
    "function ovr_confusion_for_digit(digit::Int;\n",
    "    dataset::Dict = ovr_testing_dataset,\n",
    "    models::Dict = ovr_model_dictionary)\n",
    "\n",
    "    # check inputs - make sure we have data and a model for this digit\n",
    "    @assert haskey(dataset, digit) \"Digit $(digit) not found in test dataset.\"\n",
    "    @assert haskey(models, digit) \"Digit $(digit) not found in model dictionary.\"\n",
    "\n",
    "    # setup - get the test data for this digit and augment with a bias column of ones\n",
    "    X_test, y_test = dataset[digit]; # X_test is the feature matrix, y_test is the ±1 label vector\n",
    "    number_of_examples = size(X_test, 1); # how many test examples?\n",
    "    Xb = [X_test ones(number_of_examples)]; # append a column of ones to account for the bias term\n",
    "    model = models[digit]; # retrieve the trained model for this digit\n",
    "\n",
    "    # classify - compute class probabilities and convert to ±1 predicted labels\n",
    "    P = classify(Xb, model); # P is n×2: col 1 = P(y=+1), col 2 = P(y=-1)\n",
    "    ŷ = zeros(Int64, number_of_examples); # allocate predicted label vector\n",
    "    for i ∈ 1:number_of_examples\n",
    "        ŷ[i] = (P[i,1] ≥ P[i,2]) ? 1 : -1; # predict +1 if P(y=+1) ≥ P(y=-1), else -1\n",
    "    end\n",
    "\n",
    "    # compute confusion matrix and accuracy -\n",
    "    CM = confusion(y_test, ŷ); # 2×2 confusion matrix: rows = true labels, cols = predicted labels\n",
    "    accuracy = (CM[1,1] + CM[2,2]) / length(y_test); # fraction of correct predictions (diagonal sum / total)\n",
    "\n",
    "    # return -\n",
    "    return CM, accuracy, ŷ, y_test, P\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9b1ac9",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47ad8d8",
   "metadata": {},
   "source": [
    "## Task 1: Create the One versus the Rest (OvR) Dataset\n",
    "In this task, we create one binary dataset per digit. For a chosen digit `k`, we set the label to `+1` when the image shows digit `k`, and to `-1` otherwise.\n",
    "\n",
    "> __Implementation note__:\n",
    ">\n",
    "> The logistic regression learner in this package uses a finite-difference approximation of the gradient, which is expensive for 784-pixel inputs. We need to reduce the dimensionality of the input data to make training feasible within a reasonable time frame. \n",
    "> \n",
    "> __Workaround:__ To keep the workflow interactive, we first reduce each $28\\times 28$ image to a compact feature vector and then build balanced positive/negative subsets for each digit. This preserves the OvR logic while keeping the optimization step manageable.\n",
    "\n",
    "We store the results in two dictionaries: `ovr_training_dataset` and `ovr_testing_dataset`. Each dictionary maps a digit to a tuple `(X, y)` where `X` is the feature matrix and `y` is the $\\pm 1$ label vector. These balanced datasets are the foundation for training individual classifiers in Task 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a716618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr_training_dataset, ovr_testing_dataset = let\n",
    "\n",
    "    # initialize - dictionaries mapping digit → (feature matrix, label vector) for train and test\n",
    "    training_dict = Dict{Int, Tuple{Array{Float64,2}, Vector{Int64}}}();\n",
    "    testing_dict = Dict{Int, Tuple{Array{Float64,2}, Vector{Int64}}}();\n",
    "\n",
    "    # options -\n",
    "    pool = 4; # average pooling factor: 28÷4 = 7, so each image becomes 7×7 = 49 features\n",
    "    ovr_training_positive = 200; # number of positive examples (images of the target digit) for training\n",
    "    ovr_training_negative = 200; # number of negative examples (images of all other digits) for training\n",
    "    ovr_testing_positive = 100; # number of positive examples for testing\n",
    "    ovr_testing_negative = 100; # number of negative examples for testing\n",
    "\n",
    "    # TODO: Build a balanced OvR dataset for each digit.\n",
    "    # Step 1: loop over digits in number_digit_array.\n",
    "    # Step 2: call build_ovr_dataset(...) for both training_image_dataset and testing_image_dataset.\n",
    "    # Step 3: store (Xtr, ytr) in training_dict[digit] and (Xte, yte) in testing_dict[digit].\n",
    "    # Hint: use the pool, *_positive, and *_negative options above.\n",
    "\n",
    "    #= SOLUTION (commented)\n",
    "    for digit ∈ number_digit_array\n",
    "        \n",
    "        # training dataset for this digit: 200 positive + 200 negative = 400 examples\n",
    "        Xtr, ytr = build_ovr_dataset(training_image_dataset, digit,\n",
    "            number_positive = ovr_training_positive,\n",
    "            number_negative = ovr_training_negative,\n",
    "            pool = pool\n",
    "        );\n",
    "\n",
    "        # testing dataset for this digit: 100 positive + 100 negative = 200 examples\n",
    "        Xte, yte = build_ovr_dataset(testing_image_dataset, digit,\n",
    "            number_positive = ovr_testing_positive,\n",
    "            number_negative = ovr_testing_negative,\n",
    "            pool = pool\n",
    "        );\n",
    "\n",
    "        training_dict[digit] = (Xtr, ytr); # store training data for this digit\n",
    "        testing_dict[digit] = (Xte, yte); # store testing data for this digit\n",
    "    end\n",
    "    =#\n",
    "\n",
    "    # return -\n",
    "    training_dict, testing_dict\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167e8a3b",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb4a178",
   "metadata": {},
   "source": [
    "## Task 2: Train an OvR Logistic Regression Model for Each Digit\n",
    "In this task, we train one binary logistic regression model per digit using the OvR datasets constructed in Task 1. For each digit, we fit a separate classifier that learns to recognize that digit versus all others. We keep the trained models in the `ovr_model_dictionary::Dict{Int, MyLogisticRegressionClassificationModel}` dictionary.\n",
    "\n",
    "> __Training procedure:__ \n",
    "> \n",
    "> For each digit, we append a column of ones to the feature matrix $\\mathbf{X}$ to form the augmented matrix $\\mathbf{X}_{b}\\in\\mathbb{R}^{n\\times (p+1)}$, where $p = 49$ is the number of downsampled features and the extra column captures the bias term. We then minimize the logistic loss function:\n",
    "> $$\\ell(\\mathbf{x},y,\\boldsymbol{\\theta}) = \\log\\left(1+\\exp\\left(-2\\cdot y\\cdot T\\cdot\\left(\\mathbf{x}^{\\top}\\boldsymbol{\\theta}\\right)\\right)\\right) + \\lambda\\|\\boldsymbol{\\theta}\\|_{2}^{2}$$\n",
    "> where $\\boldsymbol{\\theta}\\in\\mathbb{R}^{p+1}$ is the parameter vector, $T > 0$ is a scaling parameter controlling the steepness of the sigmoid, $\\lambda\\geq 0$ is the $L_{2}$ regularization strength, and $y\\in\\{-1,+1\\}$ is the binary label. The parameters are estimated using gradient descent with a finite-difference gradient approximation.\n",
    ">\n",
    "> * __Notation note (Temperature parameter):__ In this implementation, $T$ functions as **inverse temperature** (denoted $\\beta$ in the lecture), not physical temperature. The parameter $\\beta$ is used elsewhere in the code, hence this naming choice. Like inverse temperature in the lecture, large $T$ increases sigmoid steepness (high confidence), while small $T$ flattens it (high uncertainty).\n",
    ">\n",
    "> * __Notation note (Loss functions):__ The loss function above uses lowercase $\\ell$ to denote the **per-sample loss** for a single training example. In the lecture, uppercase $J$ denotes the **total loss** summed over all training examples: $J(\\theta) = \\sum_{i=1}^{n} \\ell(\\mathbf{x}_i, y_i, \\boldsymbol{\\theta})$. The gradient descent algorithm in the code minimizes the per-sample loss for each example during training, which is equivalent to minimizing the total loss.\n",
    "\n",
    "For time-limited scenarios, you can train once and save the parameter vectors to disk for later use.\n",
    "\n",
    "> __Offline training option:__\n",
    ">\n",
    "> We save only the learned parameter vectors `ovr_parameter_dictionary::Dict{Int, Vector{Float64}}` to disk (not the full model structs), [because `JLD2` cannot serialize anonymous functions](https://github.com/JuliaIO/JLD2.jl) such as the loss function closure. On reload, we reconstruct the model objects from the saved parameters.\n",
    "> If you change the feature-reduction factor (`pool`) or the dataset sizes, you should retrain and re-save.\n",
    "\n",
    "Let's train the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd50c534",
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr_model_dictionary = let\n",
    "\n",
    "    # initialize -\n",
    "    digits_to_train = number_digit_array; # train a binary classifier for each digit (0–9)\n",
    "    models_file = joinpath(_PATH_TO_DATA, \"ovr_logistic_parameters.jld2\"); # path to saved parameter file\n",
    "    use_pretrained = true; # if true, try to load saved parameters instead of retraining\n",
    "    train_if_missing = true; # if pretrained file is missing, train from scratch\n",
    "\n",
    "    # training hyperparameters - these control the gradient descent optimizer\n",
    "    maxiter = 150;  # maximum number of gradient descent iterations per digit\n",
    "    ϵ = 1e-3;       # convergence tolerance: stop when the loss changes by less than ϵ\n",
    "    α = 0.05;       # learning rate: step size for each gradient descent update\n",
    "    h = 1e-6;       # finite-difference step size used to approximate the gradient numerically\n",
    "    T = 1.0;        # temperature parameter: controls the steepness of the sigmoid curve\n",
    "    λ = 0.0;        # L₂ regularization strength: penalizes large parameter values (0 = no regularization)\n",
    "\n",
    "    # loss function - logistic loss with optional L₂ regularization\n",
    "    # arguments: x = feature vector, y = ±1 label, T = temperature, λ = regularization, θ = parameters\n",
    "    loss_function = (x, y, T, λ, θ) -> log(1 + exp(-2*y*T*(dot(x, θ)))) + λ*norm(θ, 2)^2;\n",
    "\n",
    "    # TODO: Implement the pretrained vs. training branches.\n",
    "    # If use_pretrained && isfile(models_file): load saved parameters and rebuild model_dictionary.\n",
    "    # Else if train_if_missing: train one model per digit, save parameters to disk, and return.\n",
    "    # Else: throw an error.\n",
    "\n",
    "    model_dictionary = Dict{Int, MyLogisticRegressionClassificationModel}();\n",
    "    #= SOLUTION (commented)\n",
    "    if (use_pretrained == true && isfile(models_file))\n",
    "        \n",
    "        # load pretrained parameter vectors from disk -\n",
    "        # we saved only the β vectors (not full model structs) because JLD2 can't serialize closures\n",
    "        @load models_file ovr_parameter_dictionary\n",
    "\n",
    "        # reconstruct model objects from saved parameters -\n",
    "        # we rebuild each model using the saved β vector and the same hyperparameters/loss defined above\n",
    "        model_dictionary = Dict{Int, MyLogisticRegressionClassificationModel}();\n",
    "        for digit ∈ digits_to_train\n",
    "            model_dictionary[digit] = build(MyLogisticRegressionClassificationModel, (\n",
    "                parameters = ovr_parameter_dictionary[digit], # learned parameter vector for this digit\n",
    "                learning_rate = α,\n",
    "                ϵ = ϵ,\n",
    "                h = h,\n",
    "                λ = λ,\n",
    "                T = T,\n",
    "                loss_function = loss_function\n",
    "            ));\n",
    "        end\n",
    "\n",
    "        # return -\n",
    "        model_dictionary\n",
    "    elseif (train_if_missing == true)\n",
    "\n",
    "        # train one binary classifier per digit -\n",
    "        model_dictionary = Dict{Int, MyLogisticRegressionClassificationModel}();\n",
    "        for digit ∈ digits_to_train\n",
    "            \n",
    "            # build augmented feature matrix with bias column -\n",
    "            # Xb = [X | 1] so the last element of θ acts as the bias (intercept) term\n",
    "            X, y = ovr_training_dataset[digit]; # get the balanced training data for this digit\n",
    "            number_of_examples = size(X, 1); # how many training examples?\n",
    "            Xb = [X ones(number_of_examples)]; # append a column of ones for the bias term\n",
    "            number_of_features = size(Xb, 2); # total features including the bias column (49 + 1 = 50)\n",
    "\n",
    "            # build model - create an untrained model with small random initial parameters\n",
    "            model = build(MyLogisticRegressionClassificationModel, (\n",
    "                parameters = 0.01*ones(number_of_features), # initialize all parameters to 0.01\n",
    "                learning_rate = α,\n",
    "                ϵ = ϵ,\n",
    "                h = h,\n",
    "                λ = λ,\n",
    "                T = T,\n",
    "                loss_function = loss_function\n",
    "            ));\n",
    "\n",
    "            # learn parameters - run gradient descent to minimize the logistic loss on this digit's data\n",
    "            trained = learn(Xb, y, model, maxiter = maxiter, verbose = true);\n",
    "            model_dictionary[digit] = trained; # store the trained model for this digit\n",
    "        end\n",
    "\n",
    "        # save only parameter vectors to disk (JLD2 cannot serialize closures) -\n",
    "        # extract the learned β vector from each trained model and save as a plain Dict{Int, Vector{Float64}}\n",
    "        mkpath(_PATH_TO_DATA); # create the data directory if it doesn't exist\n",
    "        ovr_parameter_dictionary = Dict{Int, Vector{Float64}}(); # dictionary to hold β vectors\n",
    "        for (digit, model) ∈ model_dictionary\n",
    "            ovr_parameter_dictionary[digit] = model.β; # extract the learned parameter vector\n",
    "        end\n",
    "        @save models_file ovr_parameter_dictionary # write the parameter dictionary to a JLD2 file\n",
    "\n",
    "        # return -\n",
    "        model_dictionary\n",
    "    else\n",
    "        error(\"No pretrained models found at $(models_file). Set `use_pretrained = false` to train.\");\n",
    "    end\n",
    "    =#\n",
    "\n",
    "\n",
    "    # return -\n",
    "    model_dictionary\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188750a1",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93034ea7",
   "metadata": {},
   "source": [
    "## Task 3: Evaluate One Digit vs. Rest Using a Confusion Matrix\n",
    "In this task, we validate the trained classifiers from Task 2 by selecting a digit and evaluating its OvR classifier on the test dataset. We compute predicted labels from the probability matrix and construct a confusion matrix to assess performance.\n",
    "\n",
    "> __Reminder__: In this OvR setting, the __positive__ class means _is the digit_, and the __negative__ class means _is not the digit_.\n",
    "\n",
    "Now let's select a digit and evaluate its classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2ec8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_to_test = 8; # select which digit classifier to evaluate (change this to test a different digit)\n",
    "\n",
    "# TODO: Evaluate the OvR classifier for this digit and unpack outputs.\n",
    "# CM_ovr, accuracy_ovr, ŷ_ovr, y_ovr, P_ovr = ovr_confusion_for_digit(digit_to_test);\n",
    "\n",
    "\n",
    "# initialize -\n",
    "CM_ovr = nothing; # confusion matrix for the selected digit\n",
    "accuracy_ovr = nothing; # overall accuracy for the selected digit\n",
    "ŷ_ovr = nothing; # predicted labels (+1/-1)\n",
    "y_ovr = nothing; # true labels (+1/-1)\n",
    "P_ovr = nothing; # class probability matrix\n",
    "\n",
    "#= SOLUTION (commented)\n",
    "CM_ovr, accuracy_ovr, ŷ_ovr, y_ovr, P_ovr = ovr_confusion_for_digit(digit_to_test); # evaluate the OvR classifier for this digit\n",
    "=#\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f3628a",
   "metadata": {},
   "source": [
    "The `ovr_confusion_for_digit` function returns the confusion matrix `CM_ovr::Matrix{Int64}`, the overall accuracy `accuracy_ovr::Float64`, the predicted labels `ŷ_ovr::Vector{Int64}`, the true labels `y_ovr::Vector{Int64}`, and the probability matrix `P_ovr::Matrix{Float64}`. Let's recompute and display the confusion matrix for the selected digit.\n",
    "\n",
    "> __Error analysis__\n",
    ">\n",
    "> Total mistakes (or mistake percentage) is only part of the story. We should understand whether we are biased toward false positives or false negatives. How many times did we predict an image was the target digit when it was not (false positive)? How many times did we predict \"not the digit\" when it actually was (false negative)? For this we use a confusion matrix. \n",
    ">\n",
    "> The confusion matrix for a binary classifier is typically structured as:\n",
    ">\n",
    ">|                     | **Predicted Positive** | **Predicted Negative** |\n",
    ">|---------------------|------------------------|------------------------|\n",
    ">| **Actual Positive** | True Positive (TP)     | False Negative (FN)    |\n",
    ">| **Actual Negative** | False Positive (FP)    | True Negative (TN)     |\n",
    "> \n",
    "> From the confusion matrix, we derive three key metrics:\n",
    ">\n",
    "> * __Accuracy__ is the fraction of correct predictions overall: $\\texttt{accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$. It tells us the overall success rate but can be misleading if classes are imbalanced, i.e., a classifier that predicts __negative__ for everything might achieve high accuracy on an imbalanced dataset.\n",
    ">\n",
    "> * __Precision__ answers: \"When we predict positive, how often are we right?\" $\\texttt{precision} = \\frac{TP}{TP + FP}$. High precision means fewer false alarms — if the classifier says an image is the target digit, it probably is.\n",
    ">\n",
    "> * __Recall__ (also called sensitivity) answers: \"Of all the true positives, how many did we catch?\" $\\texttt{recall} = \\frac{TP}{TP + FN}$. High recall means we are not missing instances of the target digit. Depending on the application, recall may be more critical than precision because failing to detect the target class can be costlier than a false alarm.\n",
    "\n",
    "We compute the __confusion matrix__ to get these counts. We use the [confusion(...) method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e6cf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Recompute 2×2 confusion matrix from true labels vs predicted labels.\n",
    "# CM_ovr = confusion(y_ovr, ŷ_ovr)\n",
    "\n",
    "# initialize -\n",
    "CM_ovr = nothing; # allocate variable to hold confusion matrix\n",
    "\n",
    "#= SOLUTION (commented)\n",
    "CM_ovr = confusion(y_ovr, ŷ_ovr) # recompute 2×2 confusion matrix from true labels vs predicted labels\n",
    "=#\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1b012a",
   "metadata": {},
   "source": [
    "Let's compute the accuracy, precision, and recall for our logistic regression classifier on the test dataset. We store these metrics in the `accuracy_logistic::Float64`, `precision_logistic::Float64`, and `recall_logistic::Float64` variables, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efeeac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_logistic, precision_logistic, recall_logistic = let\n",
    "\n",
    "    # TODO: Compute accuracy, precision, and recall from CM_ovr.\n",
    "    # Hint: TP = CM_ovr[1,1], TN = CM_ovr[2,2], FP = CM_ovr[2,1], FN = CM_ovr[1,2]\n",
    "\n",
    "    # initialize -\n",
    "    accuracy = nothing; # overall accuracy\n",
    "    precision = nothing; # precision\n",
    "    recall = nothing; # recall\n",
    "    \n",
    "    #= SOLUTION (commented)\n",
    "\n",
    "    # extract confusion matrix values -\n",
    "    TP = CM_ovr[1,1]; # true positives\n",
    "    TN = CM_ovr[2,2]; # true negatives\n",
    "    FP = CM_ovr[2,1]; # false positives\n",
    "    FN = CM_ovr[1,2]; # false negatives\n",
    "\n",
    "    # compute metrics -\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN); # accuracy\n",
    "    precision = TP / (TP + FP); # precision\n",
    "    recall = TP / (TP + FN); # recall\n",
    "\n",
    "    # print so the user can see -\n",
    "    println(\"Logistic Regression Accuracy: $(round(accuracy*100,digits=2))%\")\n",
    "    println(\"Logistic Regression Precision: $(round(precision*100,digits=2))%\")\n",
    "    println(\"Logistic Regression Recall: $(round(recall*100,digits=2))%\")\n",
    "\n",
    "    # return -\n",
    "    accuracy, precision, recall\n",
    "    =#\n",
    "\n",
    "\n",
    "    # return -\n",
    "    accuracy, precision, recall\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "The OvR strategy decomposes multi-class classification into independent binary problems, enabling the direct application of binary logistic regression to multi-digit recognition.\n",
    "\n",
    "> __Key Takeaways:__\n",
    ">\n",
    "> * __OvR reframes multiclass prediction as a set of binary judgments:__ This keeps the modeling simple while still supporting multi-digit recognition.\n",
    "> * __Data preparation drives feasibility:__ Balancing classes and feature reduction make finite-difference optimization practical without changing the OvR logic.\n",
    "> * __Evaluation is about behavior, not just a score:__ Confusion matrices expose which mistakes dominate, and accuracy/precision/recall make those tradeoffs explicit.\n",
    "\n",
    "This simple, scalable approach provides a foundation for extending binary classifiers to multi-class settings.\n",
    "___\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12.4",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
