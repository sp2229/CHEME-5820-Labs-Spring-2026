{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a39ab0",
   "metadata": {},
   "source": [
    "# L4b: One versus the Rest Strategy for Multi-class Classification\n",
    "In this lab, we will implement the One versus the Rest (OvR) strategy for multi-class classification using logistic regression. The OvR strategy involves training a separate binary classifier for each class, where each classifier distinguishes one class from the rest of the classes. \n",
    "\n",
    "> __Learning Objectives:__\n",
    "> \n",
    "> By the end of this lab, you will be able to:\n",
    "> \n",
    "> * __Construct OvR datasets from MNIST:__ Convert one-hot labels into binary $\\pm 1$ labels for each digit and create balanced training/testing datasets.\n",
    "> * __Train one logistic regression model per class:__ Implement the OvR training loop and store a dictionary of trained binary classifiers.\n",
    "> * __Evaluate OvR performance with confusion matrices:__ Compute and interpret confusion matrices for a selected digit vs. the rest.\n",
    "\n",
    "Let's get started!\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8f6a02",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "First, we set up the computational environment by including the `Include.jl` file and loading any needed resources.\n",
    "\n",
    "> The [`include(...)` command](https://docs.julialang.org/en/v1/base/base/#include) evaluates the contents of the input source file, `Include.jl`, in the notebook's global scope. The `Include.jl` file sets paths, loads required external packages, etc. For additional information on functions and types used in this material, see the [Julia programming language documentation](https://docs.julialang.org/en/v1/). \n",
    "\n",
    "Let's set up our code environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85f06385",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(joinpath(@__DIR__, \"Include.jl\")); # include the Include.jl file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a78e46",
   "metadata": {},
   "source": [
    "In addition to standard Julia libraries, we'll also use [the `VLDataScienceMachineLearningPackage.jl` package](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl). Check out [the documentation](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/) for more information on the functions, types, and data used in this material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0e7ad2",
   "metadata": {},
   "source": [
    "### Constants\n",
    "First, we define some constants that will be used throughout the lab. See the comments for explanations of each constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73bb461e",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_examples = 3000; # how many training examples of *each* number to include from the library\n",
    "number_of_test_examples = 500; # how many examples are we going to test on?\n",
    "number_of_training_examples = number_of_examples - number_of_test_examples; # how many training examples of *each* number to include from the library\n",
    "number_digit_array = range(0,length=10,step=1) |> collect; # numbers 0 ... 9\n",
    "number_of_rows = 28; # number of rows in the image\n",
    "number_of_cols = 28; # number of cols in the image\n",
    "number_of_pixels = number_of_rows*number_of_cols; # how many pixels do we have in the image?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78e3a98",
   "metadata": {},
   "source": [
    "### Data\n",
    "Our data consists of images of handwritten digits (0\u20139) from the [Modified National Institute of Standards and Technology (MNIST) database](https://en.wikipedia.org/wiki/MNIST_database). \n",
    "\n",
    "First, we build a training dataset of images to estimate the model parameters, stored in the `training_image_dataset::Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` variable. Next, we construct a test dataset to evaluate how well the FNN generalizes to data it has never seen, stored in the `testing_image_dataset::Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` variable.\n",
    "\n",
    "> __Data format:__ The first element of each tuple is the input data $\\mathbf{x}$ (the image pixels arranged as a vector), and the second element is the label (whether the image corresponds to digits 0\u20139).\n",
    ">\n",
    "> __Type considerations:__ The floating-point precision is `Float32` rather than the default `Float64` to reduce memory usage. The labels are [one-hot encoded](https://en.wikipedia.org/wiki/One-hot), and the input data is stored as a vector rather than a matrix (even though the original image is a $28\\times 28$ matrix of grayscale values).\n",
    "\n",
    "We load `number_of_training_examples::Int` images into the `digits_image_dictionary::Dict{Int, Array{N0f8,3}}` and then later convert these to vector format by linearizing the $28\\times 28$ matrix of grayscale values into a vector of 784 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7691eaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_image_dictionary = MyMNISTHandwrittenDigitImageDataset(number_of_examples = number_of_examples);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9609940c",
   "metadata": {},
   "source": [
    "What's in the `digits_image_dictionary::Dict{Int, Array{N0f8,3}}` variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7df7f1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAABC9JREFUaAW9wV+oZHUdAPDPOec7c//srnI3a30LMiGicPFBkkhJqAgkhR6MpB7yIYgiFEkIgghBSAgDEyJIVnqpSB/yZS2FfMkHUYgkSRDBEHRdbu7de2fmzJxzGvgJs8Nc2rfv5xOuUGEHM3SKBh0COzhQVBhQK3pFoMLc8UKykCwkC0uhWOBIMUaDiWKBA1QYo1X0ii3sYl8xQmBiXUgWkoVkUWFh0xytYg8tDhFoMSh2cYQZesU25pjYFJKFZCFZjNAqTmCKDoOVfTSKAQNqDDjCGCMcKqYI7OLIupAsJAvJolU0aNEpapxEg31UioXiGhxirugU1+EiFljYFJKFZCFZWNrCDB0ajDDFJcUn8RBux42YYAeHOIE38AQew/v+v5AsJAvJwlJl5RQmio/ip/g8bsIFRa04gRluxEOo8WscokaDuXUhWUgWkoUr1JhihtP4Eb6B09jHeXwNHf6DFm/hNpzBF/BLRaC1KSQLyUKy2MFEsY0jxa14EC3+ggfwGs7iAG9hgcCDeAR3INChdryQLCQLyWJiZaa4BrdgjjG+rNjFq1YCC0wU/0armKJBZ11IFpKFZOEKveIyrkNgodjDvmIP+xihwk3ocIDBSmdTSBaShWTRoFOM0aLH66gQuBmvoEaPfYwwwfX4KhaYo8aAEVqbQrKQLCSLDjV6DBhjhr/hTXwCL+P7eBItBsxxCp/D9RjwEnpsoUeN3rqQLCQLycLSFiZo0WAb/8DP8ShO4WHcj7/jPTyLf+E+9DjA04oKc4zRWheShWQhWVgaUGFAh05xDiM8jD3sYQ8fwf2oMcMCv8Nril7R2hSShWQhWViaIlCjQ4czeBe/wpOY4xF8D5dxElOMMcOj6K006GwKyUKykCwsVeiwsHIBI8zRosM5fAdjDHgbH8O1+CN+hr9iihqBhXUhWUgWkoWlATUq9IoevWKOs3hV8QbO4ye4HX/AzTiHb+I8RpjZFJKFZCFZ+FClqDBYOY278Bu0eAH34R3F8/gSHsdn8RQ+jf86XkgWkoVk4UODYrDuTvwWF/FPfBfvKBpcxov4Mf6MLTyBe3AKB9aFZCFZSBaVYsCgqBBo8EPFu/g6LmILMzQ4gUt4Fk/hXnwc2ziwKSQLyUKyqDBgsFKhQeAGxeO4iJM4UrRoEdjDaTQ4g6njhWQhWUgWg6LCoBjQocLrOIvHcCt+gUt4EzvYw934AT6FQ7yCGr1NIVlIFpLFgAqVYsCABXrciefwGdyNe1HjRWzjFnyAk2jxNr6FbRzZFJKFZCFZWBowWDegw/v4Cu7At/FF7OA2DIoGL+BPeA5T9I4XkoVkIVm4im1cwO/xDGpMrQs0mFkJVJhbF5KFZCFZuIq5ldbKGLv4AA0q1OhRYeF4IVlIFpKFq+iwhRE6tOjQolXMrNtCjQ4z60KykCwk+x+gryLGGYa8IwAAAABJRU5ErkJggg==",
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAABC9JREFUaAW9wV+oZHUdAPDPOec7c//srnI3a30LMiGicPFBkkhJqAgkhR6MpB7yIYgiFEkIgghBSAgDEyJIVnqpSB/yZS2FfMkHUYgkSRDBEHRdbu7de2fmzJxzGvgJs8Nc2rfv5xOuUGEHM3SKBh0COzhQVBhQK3pFoMLc8UKykCwkC0uhWOBIMUaDiWKBA1QYo1X0ii3sYl8xQmBiXUgWkoVkUWFh0xytYg8tDhFoMSh2cYQZesU25pjYFJKFZCFZjNAqTmCKDoOVfTSKAQNqDDjCGCMcKqYI7OLIupAsJAvJolU0aNEpapxEg31UioXiGhxirugU1+EiFljYFJKFZCFZWNrCDB0ajDDFJcUn8RBux42YYAeHOIE38AQew/v+v5AsJAvJwlJl5RQmio/ip/g8bsIFRa04gRluxEOo8WscokaDuXUhWUgWkoUr1JhihtP4Eb6B09jHeXwNHf6DFm/hNpzBF/BLRaC1KSQLyUKy2MFEsY0jxa14EC3+ggfwGs7iAG9hgcCDeAR3INChdryQLCQLyWJiZaa4BrdgjjG+rNjFq1YCC0wU/0armKJBZ11IFpKFZOEKveIyrkNgodjDvmIP+xihwk3ocIDBSmdTSBaShWTRoFOM0aLH66gQuBmvoEaPfYwwwfX4KhaYo8aAEVqbQrKQLCSLDjV6DBhjhr/hTXwCL+P7eBItBsxxCp/D9RjwEnpsoUeN3rqQLCQLycLSFiZo0WAb/8DP8ShO4WHcj7/jPTyLf+E+9DjA04oKc4zRWheShWQhWVgaUGFAh05xDiM8jD3sYQ8fwf2oMcMCv8Nril7R2hSShWQhWViaIlCjQ4czeBe/wpOY4xF8D5dxElOMMcOj6K006GwKyUKykCwsVeiwsHIBI8zRosM5fAdjDHgbH8O1+CN+hr9iihqBhXUhWUgWkoWlATUq9IoevWKOs3hV8QbO4ye4HX/AzTiHb+I8RpjZFJKFZCFZ+FClqDBYOY278Bu0eAH34R3F8/gSHsdn8RQ+jf86XkgWkoVk4UODYrDuTvwWF/FPfBfvKBpcxov4Mf6MLTyBe3AKB9aFZCFZSBaVYsCgqBBo8EPFu/g6LmILMzQ4gUt4Fk/hXnwc2ziwKSQLyUKyqDBgsFKhQeAGxeO4iJM4UrRoEdjDaTQ4g6njhWQhWUgWg6LCoBjQocLrOIvHcCt+gUt4EzvYw934AT6FQ7yCGr1NIVlIFpLFgAqVYsCABXrciefwGdyNe1HjRWzjFnyAk2jxNr6FbRzZFJKFZCFZWBowWDegw/v4Cu7At/FF7OA2DIoGL+BPeA5T9I4XkoVkIVm4im1cwO/xDGpMrQs0mFkJVJhbF5KFZCFZuIq5ldbKGLv4AA0q1OhRYeF4IVlIFpKFq+iwhRE6tOjQolXMrNtCjQ4z60KykCwk+x+gryLGGYa8IwAAAABJRU5ErkJg\">"
      ],
      "text/plain": [
       "\u001b[0m\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;233;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;233m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;233m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;233;48;5;232m\u2580\u001b[38;5;233;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;233m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;233;48;5;232m\u2580\u001b[38;5;233;48;5;232m\u2580\u001b[38;5;232;48;5;235m\u2580\u001b[38;5;232;48;5;247m\u2580\u001b[38;5;232;48;5;253m\u2580\u001b[38;5;232;48;5;255m\u2580\u001b[38;5;232;48;5;255m\u2580\u001b[38;5;233;48;5;255m\u2580\u001b[38;5;232;48;5;255m\u2580\u001b[38;5;232;48;5;255m\u2580\u001b[38;5;232;48;5;253m\u2580\u001b[38;5;232;48;5;242m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;233;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;233;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;233m\u2580\u001b[38;5;234;48;5;245m\u2580\u001b[38;5;246;48;5;255m\u2580\u001b[38;5;252;48;5;255m\u2580\u001b[38;5;255;48;5;255m\u2580\u001b[38;5;254;48;5;247m\u2580\u001b[38;5;255;48;5;255m\u2580\u001b[38;5;255;48;5;255m\u2580\u001b[38;5;255;48;5;253m\u2580\u001b[38;5;255;48;5;252m\u2580\u001b[38;5;255;48;5;249m\u2580\u001b[38;5;255;48;5;255m\u2580\u001b[38;5;254;48;5;255m\u2580\u001b[38;5;243;48;5;249m\u2580\u001b[38;5;233;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;233m\u2580\u001b[38;5;239;48;5;244m\u2580\u001b[38;5;255;48;5;255m\u2580\u001b[38;5;255;48;5;255m\u2580\u001b[38;5;250;48;5;232m\u2580\u001b[38;5;236;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;236;48;5;232m\u2580\u001b[38;5;235;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;243;48;5;242m\u2580\u001b[38;5;255;48;5;255m\u2580\u001b[38;5;255;48;5;251m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;246;48;5;242m\u2580\u001b[38;5;255;48;5;255m\u2580\u001b[38;5;255;48;5;255m\u2580\u001b[38;5;232;48;5;236m\u2580\u001b[38;5;233;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;233;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;234m\u2580\u001b[38;5;247;48;5;254m\u2580\u001b[38;5;255;48;5;255m\u2580\u001b[38;5;250;48;5;250m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;236;48;5;232m\u2580\u001b[38;5;253;48;5;242m\u2580\u001b[38;5;255;48;5;255m\u2580\u001b[38;5;255;48;5;255m\u2580\u001b[38;5;242;48;5;255m\u2580\u001b[38;5;232;48;5;253m\u2580\u001b[38;5;232;48;5;247m\u2580\u001b[38;5;232;48;5;245m\u2580\u001b[38;5;232;48;5;238m\u2580\u001b[38;5;233;48;5;234m\u2580\u001b[38;5;239;48;5;249m\u2580\u001b[38;5;255;48;5;255m\u2580\u001b[38;5;255;48;5;255m\u2580\u001b[38;5;244;48;5;236m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;234m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;242;48;5;232m\u2580\u001b[38;5;255;48;5;233m\u2580\u001b[38;5;255;48;5;232m\u2580\u001b[38;5;255;48;5;245m\u2580\u001b[38;5;255;48;5;255m\u2580\u001b[38;5;255;48;5;255m\u2580\u001b[38;5;255;48;5;255m\u2580\u001b[38;5;255;48;5;255m\u2580\u001b[38;5;255;48;5;255m\u2580\u001b[38;5;255;48;5;255m\u2580\u001b[38;5;246;48;5;235m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;236m\u2580\u001b[38;5;236;48;5;255m\u2580\u001b[38;5;255;48;5;255m\u2580\u001b[38;5;255;48;5;253m\u2580\u001b[38;5;255;48;5;246m\u2580\u001b[38;5;253;48;5;233m\u2580\u001b[38;5;255;48;5;239m\u2580\u001b[38;5;255;48;5;255m\u2580\u001b[38;5;251;48;5;255m\u2580\u001b[38;5;239;48;5;255m\u2580\u001b[38;5;232;48;5;238m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;233;48;5;241m\u2580\u001b[38;5;241;48;5;255m\u2580\u001b[38;5;255;48;5;255m\u2580\u001b[38;5;255;48;5;250m\u2580\u001b[38;5;249;48;5;234m\u2580\u001b[38;5;234;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;233;48;5;232m\u2580\u001b[38;5;238;48;5;232m\u2580\u001b[38;5;252;48;5;239m\u2580\u001b[38;5;255;48;5;255m\u2580\u001b[38;5;254;48;5;255m\u2580\u001b[38;5;233;48;5;248m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;233m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;233m\u2580\u001b[38;5;252;48;5;255m\u2580\u001b[38;5;255;48;5;255m\u2580\u001b[38;5;252;48;5;242m\u2580\u001b[38;5;233;48;5;232m\u2580\u001b[38;5;232;48;5;233m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;233;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;234m\u2580\u001b[38;5;247;48;5;249m\u2580\u001b[38;5;255;48;5;255m\u2580\u001b[38;5;251;48;5;253m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;251;48;5;233m\u2580\u001b[38;5;255;48;5;252m\u2580\u001b[38;5;255;48;5;255m\u2580\u001b[38;5;247;48;5;255m\u2580\u001b[38;5;236;48;5;255m\u2580\u001b[38;5;235;48;5;255m\u2580\u001b[38;5;232;48;5;250m\u2580\u001b[38;5;232;48;5;251m\u2580\u001b[38;5;234;48;5;255m\u2580\u001b[38;5;242;48;5;255m\u2580\u001b[38;5;253;48;5;255m\u2580\u001b[38;5;255;48;5;255m\u2580\u001b[38;5;255;48;5;253m\u2580\u001b[38;5;248;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;234;48;5;232m\u2580\u001b[38;5;240;48;5;232m\u2580\u001b[38;5;249;48;5;232m\u2580\u001b[38;5;255;48;5;232m\u2580\u001b[38;5;255;48;5;232m\u2580\u001b[38;5;255;48;5;232m\u2580\u001b[38;5;255;48;5;232m\u2580\u001b[38;5;255;48;5;232m\u2580\u001b[38;5;255;48;5;232m\u2580\u001b[38;5;250;48;5;232m\u2580\u001b[38;5;242;48;5;232m\u2580\u001b[38;5;235;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[0m\n",
       "\u001b[0m\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;233m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;233;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[38;5;232;48;5;232m\u2580\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "digits_image_dictionary[8][ :, :, 10] # how does the indexing work? This is the 10th example of the digit \"8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's partition the `digits_image_dictionary::Dict{Int, Array{N0f8,3}}` into training and testing datasets. We randomly select `number_of_training_examples::Int` images per digit for training, and the remaining images are used for testing. In each case, we convert the $28\\times 28$ images into vector format by linearizing the matrix into a vector of 784 pixels.\n",
    "\n",
    "Let's start with the training dataset. \n",
    "\n",
    "> __What is vectorization?__ Each $N\\times N$ image array containing grayscale values at each pixel is converted to an $N^{2}$ vector by concatenating pixel values. The image class (the digit it represents) is converted to [one-hot format](https://en.wikipedia.org/wiki/One-hot). \n",
    "\n",
    "Let's save the training data in the `training_image_dataset::Vector{Tuple{Vector{Float64}, OneHotVector{UInt32}}}` variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_image_dataset = let\n",
    "\n",
    "    # initialize -\n",
    "    training_image_dataset = Vector{Tuple{Vector{Float64}, OneHotVector{UInt32}}}();\n",
    "    for i \u2208 number_digit_array\n",
    "        Y = onehot(i, number_digit_array); # what image class is this?\n",
    "        X = digits_image_dictionary[i]; # this gets ALL images of digit \"i\"\n",
    "\n",
    "        for t \u2208 1:number_of_training_examples\n",
    "            D = reshape(transpose(X[:,:,t]) |> Matrix, number_of_pixels) |> vec; # flatten\n",
    "            training_tuple = (D,Y); # create training tuple (image data, image class)\n",
    "            push!(training_image_dataset,training_tuple);\n",
    "        end\n",
    "    end\n",
    "    training_image_dataset; # return\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load `number_of_test_examples::Int` images from the `digits_image_dictionary::Dict{Int, Array{N0f8,3}}` for testing purposes. We save the test data in the `testing_image_dataset::Vector{Tuple{Vector{Float32}, OneHotVector{UInt32}}}` variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "testing_image_dataset = let\n",
    "    \n",
    "    # initialize -\n",
    "    testing_image_dataset = Vector{Tuple{Vector{Float64}, OneHotVector{UInt32}}}()\n",
    "    \n",
    "    # main -\n",
    "    for i \u2208 number_digit_array\n",
    "        Y = onehot(i, number_digit_array); # what image class is this?\n",
    "        X = digits_image_dictionary[i]; # this gets ALL images of digit \"i\"\n",
    "        \n",
    "        for t \u2208 (number_of_training_examples+1):number_of_examples\n",
    "\n",
    "            D = reshape(transpose(X[:,:,t]) |> Matrix, number_of_pixels) |> vec; # flatten\n",
    "            testing_tuple = (D,Y); # create testing tuple (image data, image class)\n",
    "            push!(testing_image_dataset, testing_tuple);\n",
    "        end\n",
    "    end\n",
    "\n",
    "    testing_image_dataset; # return\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Create the One versus the Rest (OvR) Dataset\n",
    "In this task, we create one binary dataset per digit. For a chosen digit `k`, we set the label to `+1` when the image shows digit `k`, and to `-1` otherwise.\n",
    "\n",
    "> __Implementation note__:\n",
    ">\n",
    "> The logistic regression learner in this package uses a finite-difference approximation of the gradient. This is computationally expensive for 784-pixel images. To keep the lab interactive, we:\n",
    ">\n",
    "> * Downsample each $28\\times 28$ image into a $7\\times 7$ average-pooled image (49 features).\n",
    "> * Build a balanced dataset with a small number of positive and negative examples per digit.\n",
    "\n",
    "We store the results in two dictionaries: `ovr_training_dataset` and `ovr_testing_dataset`. Each dictionary maps a digit to a tuple `(X, y)` where `X` is the feature matrix and `y` is the $\\pm 1$ label vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "using Random\n",
    "\n",
    "# reproducibility\n",
    "Random.seed!(123);\n",
    "\n",
    "# downsample configuration (28x28 -> 7x7)\n",
    "pool = 4;\n",
    "ovr_training_positive = 200;\n",
    "ovr_training_negative = 200;\n",
    "ovr_testing_positive = 100;\n",
    "ovr_testing_negative = 100;\n",
    "\n",
    "function downsample_image(x::Vector{Float64}, pool::Int)::Vector{Float64}\n",
    "\n",
    "    # undo the earlier transpose used during vectorization\n",
    "    img = reshape(x, (number_of_cols, number_of_rows))';\n",
    "    new_rows = number_of_rows \u00f7 pool;\n",
    "    new_cols = number_of_cols \u00f7 pool;\n",
    "    pooled = zeros(Float64, new_rows, new_cols);\n",
    "\n",
    "    for i \u2208 1:new_rows\n",
    "        for j \u2208 1:new_cols\n",
    "            r = (pool*(i-1)+1):(pool*i);\n",
    "            c = (pool*(j-1)+1):(pool*j);\n",
    "            pooled[i,j] = mean(img[r,c]);\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return vec(pooled);\n",
    "end;\n",
    "\n",
    "function build_ovr_dataset(image_dataset::Vector{Tuple{Vector{Float64}, OneHotVector{UInt32}}},\n",
    "    digit::Int; number_positive::Int = 200, number_negative::Int = 200, pool::Int = 4)\n",
    "\n",
    "    # collect indices for positive/negative classes\n",
    "    pos_idx = Int[];\n",
    "    neg_idx = Int[];\n",
    "    for (i,(x,y_onehot)) \u2208 enumerate(image_dataset)\n",
    "        label = number_digit_array[argmax(y_onehot)];\n",
    "        if (label == digit)\n",
    "            push!(pos_idx, i);\n",
    "        else\n",
    "            push!(neg_idx, i);\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # sample a balanced subset\n",
    "    pos_sel = pos_idx[randperm(length(pos_idx))[1:number_positive]];\n",
    "    neg_sel = neg_idx[randperm(length(neg_idx))[1:number_negative]];\n",
    "    selected = vcat(pos_sel, neg_sel);\n",
    "    selected = selected[randperm(length(selected))];\n",
    "\n",
    "    # allocate feature matrix and label vector\n",
    "    features_per_image = (number_of_rows \u00f7 pool) * (number_of_cols \u00f7 pool);\n",
    "    X = Array{Float64,2}(undef, length(selected), features_per_image);\n",
    "    y = Array{Int64,1}(undef, length(selected));\n",
    "\n",
    "    for (k, idx) \u2208 enumerate(selected)\n",
    "        x, y_onehot = image_dataset[idx];\n",
    "        X[k,:] = downsample_image(x, pool);\n",
    "        label = number_digit_array[argmax(y_onehot)];\n",
    "        y[k] = (label == digit) ? 1 : -1;\n",
    "    end\n",
    "\n",
    "    return X, y;\n",
    "end;\n",
    "\n",
    "ovr_training_dataset, ovr_testing_dataset = let\n",
    "\n",
    "    # initialize -\n",
    "    training_dict = Dict{Int, Tuple{Array{Float64,2}, Vector{Int64}}}();\n",
    "    testing_dict = Dict{Int, Tuple{Array{Float64,2}, Vector{Int64}}}();\n",
    "\n",
    "    # build datasets for each digit\n",
    "    for digit \u2208 number_digit_array\n",
    "        Xtr, ytr = build_ovr_dataset(training_image_dataset, digit,\n",
    "            number_positive = ovr_training_positive,\n",
    "            number_negative = ovr_training_negative,\n",
    "            pool = pool\n",
    "        );\n",
    "        Xte, yte = build_ovr_dataset(testing_image_dataset, digit,\n",
    "            number_positive = ovr_testing_positive,\n",
    "            number_negative = ovr_testing_negative,\n",
    "            pool = pool\n",
    "        );\n",
    "        training_dict[digit] = (Xtr, ytr);\n",
    "        testing_dict[digit] = (Xte, yte);\n",
    "    end\n",
    "\n",
    "    # return -\n",
    "    training_dict, testing_dict\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Train an OvR Logistic Regression Model for Each Digit\n",
    "In this task, we train one binary logistic regression model per digit using the datasets from Task 1. We keep the models in the `ovr_model_dictionary::Dict{Int, MyLogisticRegressionClassificationModel}` dictionary.\n",
    "\n",
    "> __Training loop__: For each digit, we add a bias term to the feature matrix, build a `MyLogisticRegressionClassificationModel`, and learn the parameters using gradient descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Time-limited option (offline training)__:\n",
    ">\n",
    "> You can train once, save the model dictionary to disk, and reload it during the lab session.\n",
    "> If you change the downsampling factor (`pool`) or the dataset sizes, you should retrain and re-save.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "digits_to_train = number_digit_array; # change this if you want to train fewer digits\n",
    "\n",
    "models_file = joinpath(_PATH_TO_DATA, \"ovr_logistic_models.jld2\");\n",
    "use_pretrained = true; # set to false to force retraining\n",
    "train_if_missing = true; # if pretrained file is missing, should we train?\n",
    "\n",
    "ovr_model_dictionary = let\n",
    "\n",
    "    if (use_pretrained == true && isfile(models_file))\n",
    "        @load models_file ovr_model_dictionary\n",
    "        ovr_model_dictionary\n",
    "    elseif (train_if_missing == true)\n",
    "\n",
    "        # training hyperparameters\n",
    "        maxiter = 150;\n",
    "        \u03f5 = 1e-3;\n",
    "        \u03b1 = 0.05;\n",
    "        T = 1.0;\n",
    "        \u03bb = 0.0;\n",
    "\n",
    "        model_dictionary = Dict{Int, MyLogisticRegressionClassificationModel}();\n",
    "\n",
    "        for digit \u2208 digits_to_train\n",
    "            X, y = ovr_training_dataset[digit];\n",
    "            number_of_examples = size(X,1);\n",
    "            Xb = [X ones(number_of_examples)];\n",
    "            number_of_features = size(Xb,2);\n",
    "\n",
    "            loss_function = (x,y,\u03b8) -> log(1+exp(-2*y*T*(dot(x,\u03b8)))) + \u03bb*norm(\u03b8,2)^2;\n",
    "\n",
    "            model = build(MyLogisticRegressionClassificationModel, (\n",
    "                parameters = 0.01*ones(number_of_features),\n",
    "                learning_rate = \u03b1,\n",
    "                \u03f5 = \u03f5,\n",
    "                loss_function = loss_function\n",
    "            ));\n",
    "\n",
    "            trained = learn(Xb, y, model, maxiter = maxiter, verbose = true);\n",
    "            model_dictionary[digit] = trained;\n",
    "        end\n",
    "\n",
    "        # save for offline use\n",
    "        mkpath(_PATH_TO_DATA);\n",
    "        ovr_model_dictionary = model_dictionary;\n",
    "        @save models_file ovr_model_dictionary\n",
    "\n",
    "        model_dictionary\n",
    "    else\n",
    "        error(\"No pretrained models found at $(models_file). Set `use_pretrained = false` to train.\");\n",
    "    end\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Evaluate One Digit vs. Rest Using a Confusion Matrix\n",
    "In this task, we select a digit and evaluate its OvR classifier on the test dataset. We compute predicted labels from the probability matrix and then build the confusion matrix.\n",
    "\n",
    "> __Reminder__: In this OvR setting, the \u201cpositive\u201d class means \u201cis the digit,\u201d and the \u201cnegative\u201d class means \u201cis not the digit.\u201d\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "function ovr_confusion_for_digit(digit::Int;\n",
    "    dataset::Dict = ovr_testing_dataset,\n",
    "    models::Dict = ovr_model_dictionary)\n",
    "\n",
    "    @assert haskey(dataset, digit) \"Digit $(digit) not found in test dataset.\"\n",
    "    @assert haskey(models, digit) \"Digit $(digit) not found in model dictionary.\"\n",
    "\n",
    "    X_test, y_test = dataset[digit];\n",
    "    number_of_examples = size(X_test,1);\n",
    "    Xb = [X_test ones(number_of_examples)];\n",
    "    model = models[digit];\n",
    "\n",
    "    # compute class probabilities\n",
    "    P = classify(Xb, model);\n",
    "\n",
    "    # convert probabilities to \u00b11 labels\n",
    "    y\u0302 = zeros(Int64, number_of_examples);\n",
    "    for i \u2208 1:number_of_examples\n",
    "        y\u0302[i] = (P[i,1] \u2265 P[i,2]) ? 1 : -1;\n",
    "    end\n",
    "\n",
    "    CM = confusion(y_test, y\u0302);\n",
    "    accuracy = (CM[1,1] + CM[2,2]) / length(y_test);\n",
    "\n",
    "    return CM, accuracy, y\u0302, y_test, P\n",
    "end\n",
    "\n",
    "digit_to_test = 8; # change this to evaluate a different digit\n",
    "CM_ovr, accuracy_ovr, y\u0302_ovr, y_ovr, P_ovr = ovr_confusion_for_digit(digit_to_test);\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "CM_ovr = confusion(y_ovr, y\u0302_ovr)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "number_of_test_points = length(y_ovr);\n",
    "correct_prediction = CM_ovr[1,1] + CM_ovr[2,2];\n",
    "(correct_prediction/number_of_test_points) |> f-> println(\"Fraction correct: $(f) Fraction incorrect $(1-f)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "This lab demonstrated how the OvR strategy converts a multi-class problem into a set of binary logistic regression tasks, one per digit.\n",
    "\n",
    "> __Key Takeaways:__\n",
    ">\n",
    "> * **OvR decomposes multi-class into binary problems:** Each digit gets its own classifier that separates that digit from all others.\n",
    "> * **Dimensionality control matters in practice:** Downsampling and balanced subsets make gradient-based training feasible while preserving the OvR workflow.\n",
    "> * **Confusion matrices reveal digit-specific behavior:** Evaluating one digit vs. rest highlights false positives and false negatives for that digit.\n",
    "\n",
    "OvR provides a simple, scalable template for extending binary classifiers to multi-class datasets.\n",
    "___\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12.4",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}