{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b6a48b",
   "metadata": {},
   "source": [
    "# Activity: Linear Discriminant Analysis (LDA) using SVD\n",
    "In this activity, we'll use Linear Discriminant Analysis (LDA) to build a supervised low-dimensional representation of a heart disease dataset. Unlike PCA, LDA explicitly uses class labels to find directions that separate classes, and we'll compute those directions using singular value decomposition (SVD).\n",
    "\n",
    "> __Learning Objectives:__\n",
    "> \n",
    "> By the end of this activity, you should be able to:\n",
    "> \n",
    "> * __Compute class statistics and scatter matrices from clinical data:__ Build within-class and between-class scatter matrices using labeled patient data.\n",
    "> * __Use SVD to solve the LDA optimization problem:__ Convert the generalized eigenvalue problem into a standard eigenvalue problem via SVD-based whitening.\n",
    "> * __Project and interpret class separation in a reduced space:__ Visualize LDA projections and assess class separation for the `death_event` label.\n",
    "\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cddff4",
   "metadata": {},
   "source": [
    "## Review: Supervised Dimensionality Reduction with LDA\n",
    "Suppose we have a dataset $\\mathcal{D} = \\{(\\mathbf{x}_{i}, y_{i})\\}_{i=1}^{n}$ where $\\mathbf{x}_{i}\\in\\mathbb{R}^{m}$ is a feature vector and $y_{i}\\in\\{-1,1\\}$ is a class label. LDA seeks a projection vector $\\mathbf{w}$ that maximizes separation between classes while minimizing spread within each class.\n",
    "\n",
    "> __Fisher criterion and generalized eigenproblem:__ LDA chooses $\\mathbf{w}$ to maximize the ratio of between-class scatter to within-class scatter:\n",
    "> $$\n",
    "> J(\\mathbf{w}) = \\frac{\\mathbf{w}^{\\top}\\mathbf{S}_{b}\\,\\mathbf{w}}{\\mathbf{w}^{\\top}\\mathbf{S}_{w}\\,\\mathbf{w}}\n",
    "> $$\n",
    "> Maximizing $J(\\mathbf{w})$ yields the generalized eigenvalue problem:\n",
    "> $$\n",
    "> \\mathbf{S}_{b}\\,\\mathbf{w} = \\lambda\\,\\mathbf{S}_{w}\\,\\mathbf{w}\n",
    "> $$\n",
    "> where:\n",
    "> * $J(\\mathbf{w})$ is the Fisher criterion value (a scalar).\n",
    "> * $\\mathcal{D} = \\{(\\mathbf{x}_{i}, y_{i})\\}_{i=1}^{n}$ is the labeled dataset with $\\mathbf{x}_{i}\\in\\mathbb{R}^{m}$ and $y_{i}\\in\\{-1,1\\}$.\n",
    "> * $\\mathbf{w}\\in\\mathbb{R}^{m}$ is the projection direction.\n",
    "> * $\\mathbf{S}_{w}$ is the within-class scatter matrix and $\\mathbf{S}_{b}$ is the between-class scatter matrix.\n",
    "> * $\\lambda$ is the generalized eigenvalue associated with $\\mathbf{w}$.\n",
    "\n",
    "Because this lab follows a lecture on SVD, we'll solve this problem using SVD-based whitening rather than explicit matrix inversion.\n",
    "\n",
    "> __TL;DR.__ LDA is a supervised dimensionality reduction method that finds directions maximizing class separation, and SVD gives a numerically stable way to compute those directions.\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301b0f87",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "First, we set up the computational environment by including the `Include.jl` file and loading any needed resources.\n",
    "\n",
    "> The [`include(...)` command](https://docs.julialang.org/en/v1/base/base/#include) evaluates the contents of the input source file, `Include.jl`, in the notebook's global scope. The `Include.jl` file sets paths, loads required external packages, etc. For additional information on functions and types used in this material, see the [Julia programming language documentation](https://docs.julialang.org/en/v1/). \n",
    "\n",
    "Let's set up our code environment:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6765f861",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "include(joinpath(@__DIR__, \"Include.jl\")); # include the Include.jl file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d5e38e",
   "metadata": {},
   "source": [
    "In addition to standard Julia libraries, we'll also use [the `VLDataScienceMachineLearningPackage.jl` package](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl). Check out [the documentation](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/) for more information on the functions, types, and data used in this material.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81318134",
   "metadata": {},
   "source": [
    "### Data\n",
    "Next, let's load up the dataset that we will explore. The data for this lab was taken from this `2020` publication:\n",
    "* [Davide Chicco, Giuseppe Jurman: \"Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone.\" BMC Medical Informatics and Decision Making 20, 16 (2020). https://doi.org/10.1186/s12911-020-1023-5](https://pubmed.ncbi.nlm.nih.gov/32013925/)\n",
    "\n",
    "In this paper, the authors analyzed a dataset of 299 heart failure patients collected in 2015. The patients comprised 105 women and 194 men, aged between 40 and 95 years old. The dataset contains 13 features (a mixture of continuous and categorical data), which report clinical, body, and lifestyle information:\n",
    "* Some features are binary: anemia, high blood pressure, diabetes, sex, and smoking status.\n",
    "* The remaining features were continuous biochemical measurements, such as the level of the Creatinine phosphokinase (CPK) enzyme in the blood, the number of platelets, etc.\n",
    "* The class (target) variable is encoded as a binary (boolean) death event: `1` if the patient died during the follow-up period, `0` if the patient did not die during the follow-up period.\n",
    "\n",
    "We'll load this dataset as a [DataFrame instance](https://dataframes.juliadata.org/stable/) and store it in the `originaldataset::DataFrame` variable:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d01e0820",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "originaldataset = MyHeartDiseaseClinicalDataset(); # load the heart disease dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63478381",
   "metadata": {},
   "source": [
    "#### Data scaling\n",
    "LDA requires [a `Matrix`](https://docs.julialang.org/en/v1/base/arrays/#Base.Matrix-Tuple{UndefInitializer,%20Any,%20Any}), not [a `DataFrame`](https://dataframes.juliadata.org/stable/). We preprocess the data in three steps:\n",
    "\n",
    "> __Data Preprocessing:__\n",
    "> \n",
    "> * __Binary recoding:__ Convert categorical `0,1` data to `-1,1` where `0` maps to `-1` and `1` remains `1`.\n",
    "> * __Z-score normalization:__ Apply [z-score scaling](https://en.wikipedia.org/wiki/Feature_scaling) to continuous features using $x^{\\prime} = (x - \\mu)/\\sigma$ where $\\mu$ is the mean and $\\sigma$ is the standard deviation.\n",
    "> * __Label retention:__ Keep the `death_event` label because LDA is supervised.\n",
    "\n",
    "The preprocessed feature matrix is stored in `X::Matrix{Float64}`, while the label vector is stored in `y::Vector{Float64}`. We also keep the treated dataset in `dataset::DataFrame`.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "65ff009d",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "(X, y, dataset) = let\n",
    "\n",
    "    # convert 0,1 into -1,1\n",
    "    treated_dataset = copy(originaldataset);\n",
    "    transform!(treated_dataset, :anaemia => ByRow(x -> (x==0 ? -1 : 1)) => :anaemia); # maps anaemia to -1,1\n",
    "    transform!(treated_dataset, :diabetes => ByRow(x -> (x==0 ? -1 : 1)) => :diabetes); # maps diabetes to -1,1\n",
    "    transform!(treated_dataset, :high_blood_pressure => ByRow(x -> (x==0 ? -1 : 1)) => :high_blood_pressure); # maps high_blood_pressure to -1,1\n",
    "    transform!(treated_dataset, :sex => ByRow(x -> (x==0 ? -1 : 1)) => :sex); # maps sex to -1,1\n",
    "    transform!(treated_dataset, :smoking => ByRow(x -> (x==0 ? -1 : 1)) => :smoking); # maps smoking to -1,1\n",
    "    transform!(treated_dataset, :death_event => ByRow(x -> (x==0 ? -1 : 1)) => :death_event); # maps death_event to -1,1\n",
    "    \n",
    "    D = treated_dataset[:,1:end] |> Matrix; # build a data matrix from the DataFrame\n",
    "    (number_of_examples, number_of_features) = size(D);\n",
    "\n",
    "    # Which cols do we want to rescale?\n",
    "    index_to_z_scale = [\n",
    "        1 ; # 1 age\n",
    "        3 ; # 2 creatinine_phosphokinase\n",
    "        5 ; # 3 ejection_fraction\n",
    "        7 ; # 4 platelets\n",
    "        8 ; # 5 serum_creatinine\n",
    "        9 ; # 6 serum_sodium\n",
    "        12 ; # 7 time\n",
    "    ];\n",
    "\n",
    "    D\u0302 = copy(D);\n",
    "    for i \u2208 eachindex(index_to_z_scale)\n",
    "        j = index_to_z_scale[i];\n",
    "        \u03bc = mean(D[:,j]); # compute the mean\n",
    "        \u03c3 = std(D[:,j]); # compute std\n",
    "\n",
    "        # rescale -\n",
    "        for k \u2208 1:number_of_examples\n",
    "            D\u0302[k,j] = (D[k,j] - \u03bc)/\u03c3;\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # split features and labels\n",
    "    X = D\u0302[:,1:end-1]; # features\n",
    "    y = D\u0302[:,end]; # labels (death_event)\n",
    "\n",
    "    X, y, treated_dataset\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b42e2e80",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39b95a2",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086d3155",
   "metadata": {},
   "source": [
    "## Task 1: Compute Class Statistics and Scatter Matrices\n",
    "In this task, we'll compute the class means and the scatter matrices used in LDA.\n",
    "\n",
    "> __Class statistics and scatter matrices:__\n",
    "> $$\n",
    "> \\mathbf{m}_{c} = \\frac{1}{n_{c}}\\sum_{i:y_{i}=c} \\mathbf{x}_{i}\n",
    "> $$\n",
    "> $$\n",
    "> \\mathbf{m} = \\frac{1}{n}\\sum_{i=1}^{n} \\mathbf{x}_{i}\n",
    "> $$\n",
    "> $$\n",
    "> \\mathbf{S}_{w} = \\sum_{c\\in\\{-1,1\\}}\\;\\sum_{i:y_{i}=c}(\\mathbf{x}_{i}-\\mathbf{m}_{c})(\\mathbf{x}_{i}-\\mathbf{m}_{c})^{\\top}\n",
    "> $$\n",
    "> $$\n",
    "> \\mathbf{S}_{b} = \\sum_{c\\in\\{-1,1\\}} n_{c}(\\mathbf{m}_{c}-\\mathbf{m})(\\mathbf{m}_{c}-\\mathbf{m})^{\\top}\n",
    "> $$\n",
    "> where:\n",
    "> * $c\\in\\{-1,1\\}$ is the class label, and $n_{c}$ is the number of samples in class $c$.\n",
    "> * $n$ is the total number of samples.\n",
    "> * $\\mathbf{x}_{i}\\in\\mathbb{R}^{m}$ is the feature vector for sample $i$, and $y_{i}$ is its class label.\n",
    "> * $\\mathbf{m}_{c}$ is the class mean, and $\\mathbf{m}$ is the overall mean.\n",
    "> * $\\mathbf{S}_{w}$ and $\\mathbf{S}_{b}$ are the within-class and between-class scatter matrices.\n",
    "\n",
    "Let's compute $\\mathbf{S}_{w}$ and $\\mathbf{S}_{b}$ from the preprocessed data:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "0255ff7d",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "(index_pos, index_neg, m, m_pos, m_neg, S_w, S_b) = let\n",
    "\n",
    "    # split by class\n",
    "    index_pos = findall(==(1), y);\n",
    "    index_neg = findall(==(-1), y);\n",
    "    X_pos = X[index_pos, :];\n",
    "    X_neg = X[index_neg, :];\n",
    "\n",
    "    # means\n",
    "    m = mean(X, dims=1) |> vec;\n",
    "    m_pos = mean(X_pos, dims=1) |> vec;\n",
    "    m_neg = mean(X_neg, dims=1) |> vec;\n",
    "\n",
    "    # within-class scatter\n",
    "    n_features = size(X,2);\n",
    "    S_w = zeros(n_features, n_features);\n",
    "    for i \u2208 1:size(X_pos,1)\n",
    "        x = vec(X_pos[i, :]) - m_pos;\n",
    "        S_w += x * x';\n",
    "    end\n",
    "    for i \u2208 1:size(X_neg,1)\n",
    "        x = vec(X_neg[i, :]) - m_neg;\n",
    "        S_w += x * x';\n",
    "    end\n",
    "\n",
    "    # between-class scatter\n",
    "    n_pos = length(index_pos);\n",
    "    n_neg = length(index_neg);\n",
    "    \u0394_pos = m_pos - m;\n",
    "    \u0394_neg = m_neg - m;\n",
    "    S_b = n_pos*(\u0394_pos * \u0394_pos') + n_neg*(\u0394_neg * \u0394_neg');\n",
    "\n",
    "    index_pos, index_neg, m, m_pos, m_neg, S_w, S_b\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8498061",
   "metadata": {},
   "source": [
    "__Check__: Both scatter matrices should be symmetric. Let's verify this numerically:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "73241414",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "let\n",
    "\n",
    "    # initialize -\n",
    "    \u03f5 = 1e-8;\n",
    "    test_sw = norm(S_w - S_w', Inf) < \u03f5;\n",
    "    test_sb = norm(S_b - S_b', Inf) < \u03f5;\n",
    "\n",
    "    @assert test_sw \"S_w is not symmetric within tolerance!\"\n",
    "    @assert test_sb \"S_b is not symmetric within tolerance!\"\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc808e21",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe2f11d",
   "metadata": {},
   "source": [
    "## Task 2: Compute the LDA Direction using SVD\n",
    "Maximizing the Fisher criterion leads to a generalized eigenvalue problem, which we solve via SVD-based whitening.\n",
    "\n",
    "> __Generalized eigenvalue form:__\n",
    "> $$\n",
    "> \\mathbf{S}_{b}\\,\\mathbf{w} = \\lambda\\,\\mathbf{S}_{w}\\,\\mathbf{w}\n",
    "> $$\n",
    "> where $\\mathbf{w}\\in\\mathbb{R}^{m}$ is the LDA direction, $\\lambda$ is the generalized eigenvalue, and $\\mathbf{S}_{w}, \\mathbf{S}_{b}$ are the within-class and between-class scatter matrices.\n",
    ">\n",
    "> __SVD-based whitening:__ If $\\mathbf{S}_{w} = \\mathbf{U}\\,\\mathbf{\\Sigma}\\,\\mathbf{U}^{\\top}$, then\n",
    "> $$\n",
    "> \\mathbf{S}_{w}^{-1/2} = \\mathbf{U}\\,\\mathbf{\\Sigma}^{-1/2}\\,\\mathbf{U}^{\\top}\n",
    "> $$\n",
    "> and the generalized eigenvalue problem reduces to a standard eigenvalue problem in the whitened space:\n",
    "> $$\n",
    "> \\mathbf{M} = \\mathbf{S}_{w}^{-1/2}\\,\\mathbf{S}_{b}\\,\\mathbf{S}_{w}^{-1/2}\n",
    "> $$\n",
    "> where:\n",
    "> * $\\mathbf{U}$ is an orthonormal matrix of left singular vectors of $\\mathbf{S}_{w}$.\n",
    "> * $\\mathbf{\\Sigma}$ is a diagonal matrix of singular values of $\\mathbf{S}_{w}$.\n",
    "> * $\\mathbf{S}_{w}^{-1/2}$ is the whitening operator.\n",
    "> * $\\mathbf{M}$ is the whitened between-class scatter matrix.\n",
    "\n",
    "We'll compute the dominant eigenvector of $\\mathbf{M}$ using SVD and map it back to the original feature space to obtain the LDA direction $\\mathbf{w}$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "4a5804af",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "(w, \u03bb\u0302) = let\n",
    "\n",
    "    # SVD of within-class scatter\n",
    "    svd_sw = svd(S_w);\n",
    "    U = svd_sw.U;\n",
    "    \u03a3 = svd_sw.S;\n",
    "\n",
    "    # build S_w^{-1/2} with a small tolerance\n",
    "    \u03f5 = 1e-10;\n",
    "    \u03a3_inv_sqrt = Diagonal([s > \u03f5 ? 1/sqrt(s) : 0.0 for s in \u03a3]);\n",
    "    S_w_inv_sqrt = U * \u03a3_inv_sqrt * U';\n",
    "\n",
    "    # whitened between-class scatter\n",
    "    M = S_w_inv_sqrt * S_b * S_w_inv_sqrt;\n",
    "\n",
    "    # dominant direction via SVD\n",
    "    svd_m = svd(M);\n",
    "    v\u2081 = svd_m.U[:,1];\n",
    "    w = S_w_inv_sqrt * v\u2081;\n",
    "    w = w / norm(w); # normalize\n",
    "\n",
    "    \u03bb = svd_m.S[1];\n",
    "    w, \u03bb\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72389fb7",
   "metadata": {},
   "source": [
    "__Check__: Let's compute the Fisher criterion value for the LDA direction:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "81ebae8f",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "let\n",
    "\n",
    "    numerator = (w' * S_b * w)[1];\n",
    "    denominator = (w' * S_w * w)[1];\n",
    "    J = numerator/denominator;\n",
    "\n",
    "    println(\"Fisher criterion J(w) = $(round(J, digits=4))\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6263f8df",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fe64bc",
   "metadata": {},
   "source": [
    "## Task 3: Visualize the LDA Projection\n",
    "Because this dataset has two classes, LDA produces a single discriminant direction. We'll project each data point onto $\\mathbf{w}$ and visualize the class separation along that axis.\n",
    "\n",
    "> __LDA projection:__\n",
    "> $$\n",
    "> z_{i} = (\\mathbf{x}_{i}-\\mathbf{m})^{\\top}\\mathbf{w}\n",
    "> $$\n",
    "> where $z_{i}$ is the scalar LDA score for sample $i$, $\\mathbf{x}_{i}$ is the feature vector, $\\mathbf{m}$ is the overall mean vector, and $\\mathbf{w}$ is the LDA direction.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "97589d5f",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "Z = let\n",
    "\n",
    "    # center the data\n",
    "    X_centered = X .- (ones(size(X,1)) * m');\n",
    "    Z = X_centered * w; # projected scores\n",
    "    Z\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d564934",
   "metadata": {},
   "source": [
    "__Visualize__: We'll plot the projected scores for each class and mark the midpoint between class means.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "31fb5ba4",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "let\n",
    "\n",
    "    # projected scores by class\n",
    "    Z_pos = Z[index_pos];\n",
    "    Z_neg = Z[index_neg];\n",
    "\n",
    "    # class means in LDA space\n",
    "    \u03bc_pos = mean(Z_pos);\n",
    "    \u03bc_neg = mean(Z_neg);\n",
    "    \u03c4 = (\u03bc_pos + \u03bc_neg)/2; # midpoint\n",
    "\n",
    "    # visualize\n",
    "    histogram(Z_pos; bins=30, alpha=0.5, color=:red, label=\"Death Event\")\n",
    "    histogram!(Z_neg; bins=30, alpha=0.5, color=:navy, label=\"No Death Event\")\n",
    "    vline!([\u03c4], c=:gray40, lw=2, label=\"Midpoint\")\n",
    "\n",
    "    plot!(bg=\"gray95\", background_color_outside=\"white\", framestyle = :box, fg_legend = :transparent);\n",
    "    xlabel!(\"LDA projection (LD1)\", fontsize=18)\n",
    "    ylabel!(\"Count\", fontsize=18)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eecc10",
   "metadata": {},
   "source": [
    "__What do you observe?__\n",
    "\n",
    "Because LDA explicitly uses class labels, the projection typically shows clearer separation than PCA. In this heart disease dataset, the two classes shift apart along the discriminant axis, but some overlap remains because the clinical features are not perfectly separable. The midpoint line provides a simple visual threshold that highlights where misclassifications are most likely to occur.\n",
    "\n",
    "> __Interpretation__: LDA maximizes class separation in a low-dimensional space, but real clinical data often exhibit overlap due to noise, confounding factors, and feature correlations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1518a91b",
   "metadata": {},
   "source": [
    "__Performance__: We'll classify each sample by thresholding the LDA score at the midpoint between class means, then compute a confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f72404fe",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "(CM_lda, y\u0302_lda) = let\n",
    "\n",
    "    # recompute midpoint threshold in LDA space\n",
    "    Z_pos = Z[index_pos];\n",
    "    Z_neg = Z[index_neg];\n",
    "    \u03bc_pos = mean(Z_pos);\n",
    "    \u03bc_neg = mean(Z_neg);\n",
    "    \u03c4 = (\u03bc_pos + \u03bc_neg)/2;\n",
    "\n",
    "    # assign labels using the midpoint threshold\n",
    "    y\u0302 = [z >= \u03c4 ? 1 : -1 for z \u2208 Z];\n",
    "\n",
    "    # confusion matrix\n",
    "    CM = confusion(Int.(y), Int.(y\u0302));\n",
    "    CM, y\u0302\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "15eaa28a",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "let\n",
    "\n",
    "    number_of_examples = length(y);\n",
    "    correct_predictions = CM_lda[1,1] + CM_lda[2,2];\n",
    "    (correct_predictions/number_of_examples) |> f -> println(\"Fraction correct: $(f) Fraction incorrect $(1-f)\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b52fd7",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a8f234",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This activity applied Linear Discriminant Analysis to a heart disease dataset and used SVD-based whitening to compute the discriminant direction.\n",
    "\n",
    "> __Key Takeaways:__\n",
    "> \n",
    "> * **LDA is supervised dimensionality reduction:** It uses class labels to find directions that maximize between-class variance while minimizing within-class variance.\n",
    "> * **SVD provides a stable route to LDA solutions:** Whitening the within-class scatter matrix with SVD converts the generalized eigenvalue problem into a standard eigenvalue problem.\n",
    "> * **LDA projections reveal class separation:** Even with overlap, the LDA axis exposes the dominant direction that best separates the `death_event` classes.\n",
    "\n",
    "LDA complements PCA by focusing on class separability rather than variance alone, making it a useful tool when labels are available.\n",
    "\n",
    "___\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12.4",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}