{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae26db5-5215-4b26-b425-521f797a9b9a",
   "metadata": {},
   "source": [
    "# Lab 3d: The XOR Problem\n",
    "In this lab, we'll represent datasets with different separability properties, train Perceptron and Logistic Regression classifiers on both linearly separable and non-linearly separable data, and analyze their performance on [the XOR dataset](https://en.wikipedia.org/wiki/Exclusive_or) to understand the fundamental limitations of linear classifiers.\n",
    "\n",
    "* __Backstory__: In 1969, AI researchers [Marvin Minsky](https://en.wikipedia.org/wiki/Marvin_Minsky) and [Seymour Papert](https://en.wikipedia.org/wiki/Seymour_Papert) published a book called [Perceptrons](https://en.wikipedia.org/wiki/Perceptrons_(book)), in which they argued that [the perceptron built by Rosenblatt](https://en.wikipedia.org/wiki/Perceptron) was incapable of learning certain functions, one of those being [the XOR function](https://en.wikipedia.org/wiki/Exclusive_or). Soon after [Perceptrons](https://en.wikipedia.org/wiki/Perceptrons_(book)) was published, the first [AI winter](https://en.wikipedia.org/wiki/AI_winter) began. Are these events correlated, maybe!\n",
    "\n",
    "First, let's build and analyze datasets that _should work_, i.e., they are (nearly) linearly separable, and then we'll explore datasets that we know for sure _will not work_, i.e., they are not linearly separable, and see what happens.  \n",
    "\n",
    "> __Learning Objectives:__\n",
    ">\n",
    "> By the end of this lab, you will be able to:\n",
    ">\n",
    "> * __Understand the concept of linear separability and its impact on classifier performance:__ Recognize how dataset geometry determines whether linear classifiers can achieve perfect separation and why certain patterns like XOR are inherently non-separable by linear decision boundaries.\n",
    "> * __Build and train Perceptron and Logistic Regression models:__ Implement both classification algorithms, apply online learning and gradient descent methods to estimate model parameters from training data, and use trained models to predict labels on unseen test data.\n",
    "> * __Evaluate classifier performance using confusion matrices:__ Compute and interpret confusion matrices to quantify classification accuracy, identify misclassified samples, and understand the relationship between data separability and model performance limitations.\n",
    "\n",
    "Let's get started!\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af49309-7d3d-47c7-a48d-af09abe2976d",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "First, we set up the computational environment by including the `Include.jl` file and loading any needed resources.\n",
    "\n",
    "> The [`include(...)` command](https://docs.julialang.org/en/v1/base/base/#include) evaluates the contents of the input source file, `Include.jl`, in the notebook's global scope. The `Include.jl` file sets paths, loads required external packages, etc. For additional information on functions and types used in this material, see the [Julia programming language documentation](https://docs.julialang.org/en/v1/). \n",
    "\n",
    "Let's set up our code environment:\n",
    "\n",
    "__Instructions:__ Uncomment the code below to include `Include.jl` and set up the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78b32458-bba8-4073-b4ea-39b33c95d5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(joinpath(@__DIR__, \"Include.jl\")); # include the Include.jl file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3213b5",
   "metadata": {},
   "source": [
    "In addition to standard Julia libraries, we'll also use [the `VLDataScienceMachineLearningPackage.jl` package](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl). Check out [the documentation](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/) for more information on the functions, types, and data used in this material.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3917cbd8-6f10-4079-92a3-8429d35ced1d",
   "metadata": {},
   "source": [
    "### Constants\n",
    "Next, let's set some constants we'll need for the data generation logic below. Please look at the comment next to the constant for a description of what it is, permissible values, etc.\n",
    "\n",
    "__Instructions:__ Uncomment the code below to define the constants used for data generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "861bf71e-73d3-4c38-8394-9ce4ab95ad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_label_one = 1000; # number of points in cloud 1 (must be ≥ 2)\n",
    "number_label_two = 1000; # number of points in cloud 2 (must be ≥ 2)\n",
    "total_number_of_points = (number_label_one + number_label_two);\n",
    "number_of_features = 3; # features: (x,y,l), where l is a generated label; see below.\n",
    "c̄₁ = (0.0, 0.0); # center for the cloud: fixed\n",
    "θ = 60*(π/180); # rotation angle (radians)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc3a812-6f24-4130-bffc-77019b4359f8",
   "metadata": {},
   "source": [
    "Finally, let's set up the color dictionary to visualize the classification datasets. The keys of the `my_color_dictionary::Dict Int64, RGB` dictionary are class labels, i.e., $ y\\in\\{1,-1\\}$ while the values are the colors mapped to that label.\n",
    "\n",
    "__Instructions:__ Uncomment the code below to define the label color dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e7243d3-ecd5-4975-822b-cd03cca324bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_color_dictionary = Dict{Int64,RGB}();\n",
    "my_color_dictionary[1] = colorant\"#03045e\"; # color for Label = 1\n",
    "my_color_dictionary[-1] = colorant\"#e36414\"; # color for Label = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc0acca",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Before generating our synthetic dataset, we need to define helper functions for data generation and quality assessment. These functions will generate circular data clouds, create circle boundaries for visualization, and compute silhouette scores to evaluate clustering quality.\n",
    "\n",
    "__Instructions:__ Uncomment the code below to define `fixedcircle(...)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dff39e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    fixedcircle(center::Tuple{Float64,Float64}; number_of_points::Int = 100, radius::Float64 = 1.0) -> Array{Float64,2}\n",
    "\n",
    "Generate random data points around a center point that lie on a circle.\n",
    "\n",
    "### Arguments\n",
    "- `center::Tuple{Float64, Float64}`: The center point around which the data points will be generated.\n",
    "- `number_of_points::Int = 100`: The number of data points to generate.\n",
    "- `radius::Float64 = 1.0`: The radius of the circle around the center point. Default value is 1.0.\n",
    "\n",
    "### Returns\n",
    "- A 2D array of data points. The first two columns are the x and y coordinates of the data points.\n",
    "\"\"\"\n",
    "function fixedcircle(center::Tuple{Float64,Float64}; \n",
    "    number_of_points::Int = 100, radius::Float64 = 1.0)::Array{Float64,2}\n",
    "\n",
    "    # initialize -\n",
    "    data = zeros(number_of_points, 2);\n",
    "    θ = range(0, 2π, length=number_of_points);\n",
    "\n",
    "    # generate the data -\n",
    "    for i ∈ 1:number_of_points\n",
    "        # generate random data points -\n",
    "        data[i,1] = center[1] + radius * cos(θ[i]); # x\n",
    "        data[i,2] = center[2] + radius * sin(θ[i]); # y\n",
    "    end\n",
    "\n",
    "    # return -\n",
    "    return data;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78a53e8",
   "metadata": {},
   "source": [
    "The `fixedcircle(...)` function generates evenly spaced points on a circle boundary for visualization purposes. We'll use this to overlay circle boundaries on our plots to show the true extent of each data cloud.\n",
    "\n",
    "Next, let's define the data generation function:\n",
    "\n",
    "__Instructions:__ Uncomment the code below to define `generatedatacloud(...)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67fbbfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    generatedatacloud(center::Tuple{Float64,Float64}; number_of_points::Int = 100, radius::Float64 = 1.0, label::Int64 = 1) -> Array{Float64,2}\n",
    "\n",
    "Generate random data points around a center point that have a label, and radius less than equal to the given radius.\n",
    "\n",
    "### Arguments\n",
    "- `center::Tuple{Float64,Float64}`: The center point around which the data points will be generated.\n",
    "- `number_of_points::Int = 100`: The number of data points to generate.\n",
    "- `radius::Float64 = 1.0`: The radius of the circle around the center point. Default value is 1.0.\n",
    "- `label::Int64 = 1`: The label to assign to the data points. Default value is 1.\n",
    "\n",
    "### Returns\n",
    "- A 2D array of data points. The first two columns are the x and y coordinates of the data points, and the third column is the label.\n",
    "\"\"\"\n",
    "function generatedatacloud(center::Tuple{Float64,Float64}; \n",
    "    number_of_points::Int = 100, radius::Float64 = 1.0, label::Int64 = 1)::Array{Float64,2}\n",
    "\n",
    "    # initialize -\n",
    "    data = zeros(number_of_points, 3);\n",
    "\n",
    "    # generate the data -\n",
    "    for i ∈ 1:number_of_points\n",
    "        \n",
    "        θ = rand() * 2π; # random angle\n",
    "        r = rand() * radius; # random radius\n",
    "\n",
    "        # generate random data points -\n",
    "        data[i,1] = center[1] + r * cos(θ); # x\n",
    "        data[i,2] = center[2] + r * sin(θ); # y\n",
    "        data[i,3] = label; # label\n",
    "    end\n",
    "\n",
    "    # return -\n",
    "    return data;\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a14d49-485e-4db4-8866-b314d028d42a",
   "metadata": {},
   "source": [
    "### Data\n",
    "We'll use [the Perceptron (Rosenblatt, 1957)](https://en.wikipedia.org/wiki/Perceptron) and [Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) to classify datasets, including [the XOR dataset](https://en.wikipedia.org/wiki/Exclusive_or) that we construct. First, we'll generate a master dataset (which may or may not be linearly separable), and then we'll split it into `training` and `test` subsets.\n",
    "* __Training data__: Training datasets are collections of labeled data used to teach machine learning models, allowing these tools to learn patterns and relationships within the data. In our case, we'll use the training data to estimate the classifier parameters $\\beta$.\n",
    "* __Test data__: Test datasets, on the other hand, are separate sets of labeled data used to evaluate the performance of trained models on unseen examples, providing an unbiased assessment of the _model's generalization capabilities_.\n",
    "\n",
    "Let's start with the master dataset `D::Array{Float64,2}`. This dataset will have two continuous features $\\mathbf{x}\\in\\mathbb{R}^{2}$ and a categorical label $y\\in\\{-1,1\\}$. We'll build a label function $L:\\mathbb{R}\\times\\mathbb{R}\\to\\{\\text{true, false}\\}$ (we can change this function around to get different labeling patterns):\n",
    "\n",
    "__Instructions:__ Uncomment exactly one labeling function below to choose the labeling pattern for `D`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f04981-afa6-4add-b1e0-ad9d9d4ae8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=\n",
    "# TODO: uncomment this logic for different labeling patterns\n",
    "# L(x,y) = (y ≥ 0) ? true : false; # rotated half circle (linearly separable)\n",
    "L(x,y) = (x ≥ 0) && (y ≥ 0) ? true : false; # wedge pattern (not linearly separable)\n",
    "# L(x,y) = xor(x ≥ 0,y ≥ 0); # XOR pattern alternating pie wedges (not linearly separable)\n",
    "=#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2390535-88f3-4047-ac22-3ab0708b651b",
   "metadata": {},
   "source": [
    "Generate the master dataset `D`:\n",
    "\n",
    "__Instructions:__ Uncomment the code below to generate the master dataset `D`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "503ba9b1-4aea-4141-a29a-87d1416d8ceb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#=\n",
    "D = let\n",
    "    \n",
    "    # initialize -\n",
    "    D = Array{Float64,2}(undef, total_number_of_points, number_of_features);\n",
    "    s₁ = generatedatacloud(c̄₁, number_of_points = number_label_one, label=0); # generate label 1 data\n",
    "    s₂ = generatedatacloud(c̄₁, number_of_points = number_label_two, label=0); # generate label 2 data\n",
    "\n",
    "    # mix s₁, s₂ together (randomly)\n",
    "    tmp = vcat(s₁,s₂)\n",
    "    random_perm_index_vector = randperm(total_number_of_points);\n",
    "    for i ∈ eachindex(random_perm_index_vector)\n",
    "        k = random_perm_index_vector[i]; # get the from col -\n",
    "        for j ∈ 1:number_of_features\n",
    "            D[i,j] = tmp[k,j];\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # compute the label by taking the XOR, OR, AND, etc functions\n",
    "    for i ∈ 1:total_number_of_points\n",
    "        flag = L(D[i,1],D[i,2]);\n",
    "        if (flag == true)\n",
    "            D[i,3] = 1\n",
    "        else\n",
    "            D[i,3] = -1\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # finally, let's rotate the data a bit (counter-clockwise)\n",
    "    R = [\n",
    "        cos(θ) -sin(θ) ;\n",
    "        sin(θ) cos(θ) ;\n",
    "    ];\n",
    "\n",
    "    # keep the label, but apply the rotation the (x,y) data\n",
    "    D̂ = copy(D);\n",
    "    for i ∈ 1:total_number_of_points\n",
    "        x̂ = R*D[i,1:2];\n",
    "        D̂[i,1] = x̂[1];\n",
    "        D̂[i,2] = x̂[2];\n",
    "    end\n",
    "    \n",
    "    D̂ # return the data\n",
    "end;\n",
    "=#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b920e5a9-440a-48b8-85a8-c2132eec7358",
   "metadata": {},
   "source": [
    "#### Visualize dataset `D`\n",
    "`Uncomment` the code block below to see how we plotted the dataset `D` which contains two continuous features and a label. The color indicates the label.\n",
    "\n",
    "> __Summary__: We will get a different pattern of $\\pm{1}$ labels depending on the labeling function logic $L(x_{1},x_{2})$ we used. The dark blue dots represent label `1`, while the orange data represents label `-1`. Our classifier should be able to learn the mapping between the features and the labels for linearly separable datasets.\n",
    "\n",
    "So what do we see?\n",
    "\n",
    "__Instructions:__ Uncomment the code below to visualize the dataset `D`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9a9ec21-6a1b-46e6-b9ae-eac967bfe340",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#=\n",
    "let\n",
    "\n",
    "    dataset = D; # what dataset am I looking at?\n",
    "    p = plot(bg=\"gray95\", background_color_outside=\"white\", framestyle = :box, fg_legend = :transparent); # make an empty plot\n",
    "\n",
    "    # plot label = 1\n",
    "    testlabel = 1;\n",
    "    i = findfirst(label -> label == testlabel,  dataset[:,3])\n",
    "    c = my_color_dictionary[testlabel]\n",
    "    scatter!([dataset[i,1]], [dataset[i,2]], label=\"Label: $(testlabel)\", c=c)\n",
    "\n",
    "    # plot label = -1\n",
    "    testlabel = -1;\n",
    "    i = findfirst(label -> label == testlabel,  dataset[:,3])\n",
    "    c = my_color_dictionary[testlabel]\n",
    "    scatter!([dataset[i,1]], [dataset[i,2]], label=\"Label: $(testlabel)\", c=c)\n",
    "\n",
    "    # plot all points\n",
    "    for i ∈ 1:total_number_of_points\n",
    "        label = dataset[i,3]; # label\n",
    "        c = my_color_dictionary[label]\n",
    "        scatter!([dataset[i, 1]], [dataset[i, 2]], label=\"\", mec=:navy, c=c)\n",
    "    end\n",
    "    \n",
    "    xlabel!(\"Feature 1 (AU)\", fontsize=18);\n",
    "    ylabel!(\"Feature 2 (AU)\", fontsize=18);\n",
    "end\n",
    "=#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63663e3-b83d-4949-b524-778057f436c5",
   "metadata": {},
   "source": [
    "Next, let's split that dataset `D` into `training` and `test` subsets. We do this randomly, where the `number_of_training_examples::Int64` variable specifies the number of training points. The `training::Array{Float64,2}` data will be used to estimate the model parameters, and `test::Array{Float64,2}` will be used for model testing.\n",
    "\n",
    "__Instructions:__ Uncomment the code below to split `D` into `training` and `test` sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ef6d985-ebcb-486f-a7ce-f9117a438c7a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#=\n",
    "training, test = let\n",
    "\n",
    "    number_of_training_examples = 1200; # we make this number of\n",
    "    number_of_examples = total_number_of_points;\n",
    "    full_index_set = range(1,stop=number_of_examples,step=1) |> collect |> Set;\n",
    "    \n",
    "    # build index sets for training and testing\n",
    "    training_index_set = Set{Int64}();\n",
    "    should_stop_loop = false;\n",
    "    while (should_stop_loop == false)\n",
    "        i = rand(1:number_of_examples);\n",
    "        push!(training_index_set,i);\n",
    "\n",
    "        if (length(training_index_set) == number_of_training_examples)\n",
    "            should_stop_loop = true;\n",
    "        end\n",
    "    end\n",
    "    test_index_set = setdiff(full_index_set,training_index_set);\n",
    "\n",
    "    # build the test and train datasets -\n",
    "    training = D[training_index_set |> collect,:];\n",
    "    test = D[test_index_set |> collect,:];\n",
    "\n",
    "    # return\n",
    "    training, test\n",
    "end;\n",
    "=#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0b97fc",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6120fb0-030b-4412-9bd9-51d4c0e1e440",
   "metadata": {},
   "source": [
    "## Task 1: Build a Perceptron Classification Model and Learn the Parameters\n",
    "In this task, we'll build a model of our Perceptron classifier model and train the model using an online learning method. \n",
    "Here is the Perceptron learning algorithm pseudocode: \n",
    "\n",
    "__Initialize__: Given a linearly separable dataset $\\mathcal{D} = \\left\\{(\\mathbf{x}_{1},y_{1}),\\dotsc,(\\mathbf{x}_{n},y_{n})\\right\\}$, maximum iterations $T$, and maximum mistakes $M$ (e.g., $M=1$), initialize parameter vector $\\theta = \\left(\\mathbf{w}, b\\right)$ to small random values and set loop counter $t\\gets{0}$.\n",
    "\n",
    "> **Rule of thumb for $T$**: Set $T = 10n$ to $100n$, where $n$ is the number of training examples. Convergence is usually faster for linearly separable data.\n",
    "\n",
    "While $\\texttt{true}$ __do__:\n",
    "1. Initialize mistakes $\\texttt{mistakes} = 0$.\n",
    "2. For each training example $(\\mathbf{x}, y) \\in \\mathcal{D}$: compute $y\\;\\left(\\theta^{\\top}\\;\\mathbf{x}\\right)\\leq{0}$. If true, the example is misclassified. Update $\\theta \\gets \\theta + y\\;\\mathbf{x}$ and increment $\\texttt{mistakes} \\gets \\texttt{mistakes} + 1$.\n",
    "3. After processing all examples, if $\\texttt{mistakes} \\leq {M}$ or $t \\geq T$, exit. Otherwise, increment $t \\gets t + 1$ and repeat from step 1.\n",
    "\n",
    "__Convergence__: If the dataset $\\mathcal{D}$ is linearly separable, the Perceptron converges to a separating hyperplane in finite iterations. If not linearly separable, the Perceptron may not converge. \n",
    "\n",
    "> __Learning__:\n",
    ">\n",
    "> We _learn_ the model parameters [using the `learn(...)` method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/#VLDataScienceMachineLearningPackage.learn), which takes the training features array `X,` the training labels vector `y`, and the problem instance and returns an updated problem instance holding the updated parameters. \n",
    "\n",
    "We'll store information about the trained model in a variable called `perceptron_model::MyPerceptronClassificationModel`.\n",
    "\n",
    "__Instructions:__ Uncomment the code below to build and train the perceptron model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b466172-602e-4920-adf7-4102a04f1db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=\n",
    "perceptron_model = let\n",
    "    \n",
    "    # setup\n",
    "    D = training; # what dataset are we going to use?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(D,2); # how many features do we have (cols)?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)]; # features, what??\n",
    "    y = D[:,end]; # output: this is the target data (label)\n",
    "    \n",
    "    # build an initial model\n",
    "    model = build(MyPerceptronClassificationModel, (\n",
    "        parameters = ones(number_of_features),\n",
    "        mistakes = 0 # willing to live with m mistakes\n",
    "    ));\n",
    "\n",
    "    # train the model -\n",
    "    trainedmodel = learn(X,y,model, maxiter = 1000, verbose = true);\n",
    "\n",
    "    # return -\n",
    "    trainedmodel;\n",
    "end;\n",
    "=#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8144eb6b-0a7d-4a19-a12f-8e91c82750ff",
   "metadata": {},
   "source": [
    "__Inference__: Now that we have parameters estimated from the `training` data, we can use those parameters on the `test` dataset to see how well the model can differentiate between classes on data it has never seen. \n",
    "\n",
    "We run the classification operation on the (unseen) test data [using the `classify(...)` method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/#VLDataScienceMachineLearningPackage.classify). This method takes a feature array `X` and the (trained) model instance. It returns the estimated labels. We store the actual (correct) label in the `y_perceptron::Array{Int64,1}` vector, while the model predicted label is stored in the `ŷ_perceptron::Array{Int64,1}` array.\n",
    "\n",
    "__Instructions:__ Uncomment the code below to compute perceptron predictions on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "631c058f-7dc1-4848-971e-0f4190476912",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=\n",
    "ŷ_perceptron,y_perceptron = let\n",
    "\n",
    "    D = test; # what dataset are going to use?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    number_of_features = size(D,2); # how many features do we have (cols)?\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)]; # features: need to add a 1 to each row (for bias), after removing the label\n",
    "    y = D[:,end]; # output: this is the *actual* target data (label)\n",
    "\n",
    "    # compute the estimated labels -\n",
    "    ŷ = classify(X,perceptron_model)\n",
    "\n",
    "    # return -\n",
    "    ŷ,y\n",
    "end;\n",
    "=#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076fab6c-776d-4cb4-b7ab-f789f0e0cab5",
   "metadata": {},
   "source": [
    "__Visualize the misses__. Using the test dataset, let's show (with gray circles) which samples our classifier is unable to predict the label correctly, i.e., where we miss the label.\n",
    "\n",
    "__Instructions:__ Uncomment the code below to visualize perceptron classification misses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d86717ea-39e1-4918-8cef-c2daa78fa57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=\n",
    "let\n",
    "\n",
    "    model = perceptron_model; # which model am I using?\n",
    "    dataset = test; # what dataset am I looking at?\n",
    "    caselabel = \"test\";\n",
    "    actual = y_perceptron;\n",
    "    predicted = ŷ_perceptron;\n",
    "    number_of_points = size(dataset,1); # number of rows\n",
    "    p = plot(bg=\"gray95\", background_color_outside=\"white\", framestyle = :box, fg_legend = :transparent); # make an empty plot\n",
    "    \n",
    "    # plot label = 1\n",
    "    testlabel = 1;\n",
    "    i = findfirst(label -> label == testlabel,  dataset[:,3])\n",
    "    c = my_color_dictionary[testlabel]\n",
    "    scatter!([dataset[i,1]], [dataset[i,2]], label=\"Label: $(testlabel)\", c=c)\n",
    "\n",
    "    # plot label = -1\n",
    "    testlabel = -1;\n",
    "    i = findfirst(label -> label == testlabel,  dataset[:,3])\n",
    "    c = my_color_dictionary[testlabel]\n",
    "    scatter!([dataset[i,1]], [dataset[i,2]], label=\"Label: $(testlabel)\", c=c)\n",
    "\n",
    "    # let's draw the separating hyperplane (in our case, a line)\n",
    "    p = model.β;\n",
    "    number_of_plane_points = 200;\n",
    "    x₂ = zeros(number_of_plane_points);\n",
    "    x₁ = range(-1,stop=1,length = number_of_plane_points) |> collect;\n",
    "    for i ∈ 1:number_of_plane_points\n",
    "        x₂[i] = -1*((p[1]/p[2])*x₁[i] + p[3]/p[2]);\n",
    "    end\n",
    "    plot!(x₁,x₂,lw=2, c=:green, label=\"Learned boundary\")\n",
    "    \n",
    "    # data -\n",
    "    for i ∈ 1:number_of_points\n",
    "        actuallabel = actual[i]; # actual label\n",
    "        testlabel = predicted[i]; # predited label\n",
    "\n",
    "        c = :gray60;\n",
    "        if (actuallabel == testlabel)\n",
    "            c = my_color_dictionary[actuallabel]\n",
    "        end\n",
    "        scatter!([dataset[i, 1]], [dataset[i, 2]], label=\"\", mec=:navy, c=c)\n",
    "    end\n",
    "\n",
    "    title!(\"Perceptron: $(caselabel)\", fontsize=18)\n",
    "    xlabel!(\"Feature 1 (AU)\", fontsize=18);\n",
    "    ylabel!(\"Feature 2 (AU)\", fontsize=18);\n",
    "end\n",
    "=#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f9eec4",
   "metadata": {},
   "source": [
    "__Performance__: Once we have has converged (or exhasted our iterations), we can evaluate the binary classifier's performance using various metrics. The central idea is to compare the predicted labels $\\hat{y}_{i}$ to the actual labels $y_{i}$ in the `test` dataset and measure wins (when the label is the same) and losses (label is different). This is easily represented in [the confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
    "\n",
    "> __Error analysis using the confusion matrix__\n",
    ">\n",
    "> Total mistakes (or mistake percentage) is only part of the story. We should understand whether we are biased toward false positives or false negatives. How many times did we predict a banknote was forged when it was genuine (false positive)? How many times did we predict genuine when it was forged (false negative)? For this we use a confusion matrix. \n",
    ">\n",
    "> The confusion matrix for a binary classifier is typically structured as:\n",
    ">\n",
    ">|                     | **Predicted Positive** | **Predicted Negative** |\n",
    ">|---------------------|------------------------|------------------------|\n",
    ">| **Actual Positive** | True Positive (TP)     | False Negative (FN)    |\n",
    ">| **Actual Negative** | False Positive (FP)    | True Negative (TN)     |\n",
    "\n",
    "We compute the __confusion matrix__ to get these counts. We use the [confusion(...) method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/#VLDataScienceMachineLearningPackage.confusion), which takes actual labels and estimated labels, returning the confusion matrix.\n",
    "\n",
    "__Instructions:__ Uncomment the code below to compute the perceptron confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6460a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=\n",
    "CM_perceptron = confusion(y_perceptron, ŷ_perceptron)\n",
    "=#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bd8bd3",
   "metadata": {},
   "source": [
    "Finally, we can compute the overall error rate for the perceptron (or other performance metrics) using values from [the confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix). The [`confusion(...)` method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/#VLDataScienceMachineLearningPackage.confusion) takes the actual labels and the computed labels and returns the confusion matrix.\n",
    "\n",
    "__Instructions:__ Uncomment the code below to compute the perceptron accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b655c73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=\n",
    "number_of_test_points = length(y_perceptron);\n",
    "correct_prediction_perceptron = CM_perceptron[1,1] + CM_perceptron[2,2];\n",
    "(correct_prediction_perceptron/number_of_test_points) |> f-> println(\"Fraction correct: $(f) Fraction incorrect $(1-f)\")\n",
    "=#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40408d5",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c5a28f-e15f-4e82-a956-a2a7eb8636a4",
   "metadata": {},
   "source": [
    "## Task 2: Build and Train Logistic Regression Classification Model\n",
    "In this task, we build and train a [Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) classifier using the training data, and then challenge this classifier using the `test` dataset. Unlike the Perceptron model, which outputs the class label directly, logistic regression models compute the _probability_ that a given input belongs to a particular class based on the input features. The training method is also different, we have to iteratively estimate the model parameters (in this case using [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)).\n",
    "\n",
    "We implemented [the `MyLogisticRegressionClassificationModel` type](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/types/#VLDataScienceMachineLearningPackage.MyLogisticRegressionClassificationModel), which contains data required to solve the logistic regression problem, i.e., parameters, the learning rate, a stopping tolerance parameter $\\epsilon$, and a loss (objective) function that we want to minimize. \n",
    "\n",
    "> __Technical note__: We approximated the gradient calculation using [a forward finite difference](https://en.wikipedia.org/wiki/Finite_difference). This is generally not a great idea. This is one of my super pet peeves with gradient descent; computing the gradient is (usually) a hassle. Typically, we have to do at least two function evaluations to approximate the gradient well. Why do finite diference? It is easy to implement.\n",
    "\n",
    "In the code below, we [build a `model::MyLogisticRegressionClassificationModel` instance using a `build(...)` method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/factory/#VLDataScienceMachineLearningPackage.build). The model instance initially has a random guess for the classifier parameters. We use gradient descent to refine that guess [using the `learn(...)` method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/#VLDataScienceMachineLearningPackage.learn), which returns an updated model instance (with the best parameters that we found so far). We return the updated model instance and save it in the `model_logistic::MyLogisticRegressionClassificationModel` variable.\n",
    "\n",
    "__Instructions:__ Uncomment the code below to build and train the logistic regression model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf482f20-69dc-479d-99da-4ec5e18ca0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=\n",
    "model_logistic = let\n",
    "\n",
    "    # data -\n",
    "    D = training; # what dataset are we going to use?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)]; # features: remove label and add bias term\n",
    "    y = D[:,end]; # output: this is the target data (label)\n",
    "    number_of_features = size(X,2); # total features including bias\n",
    "    T = 1.0; # inverse temperature for logistic regression\n",
    "    h = 1e-6; # step size for finite difference gradient approximation\n",
    "    λ = 0.0; # regularization parameter\n",
    "    ϵ = 1e-6; # tolerance for convergence\n",
    "    maxiter = 20000; # maximum number of iterations for learning\n",
    "\n",
    "    # model\n",
    "    model = build(MyLogisticRegressionClassificationModel, (\n",
    "        parameters = 0.01*ones(number_of_features), # initial value for the parameters: these will be updated\n",
    "        learning_rate = 0.005, # you pick this\n",
    "        ϵ = ϵ, # you pick this (this is the tolerance for convergence)\n",
    "        h = h, # you pick this (this is the step size for finite difference gradient approximation)\n",
    "        λ = λ, # regularization parameter\n",
    "        T = T, # inverse temperature for logistic regression\n",
    "        loss_function = (x,y,T,λ,θ) -> log(1+exp(-2*y*T*(dot(x,θ)))) + λ*norm(θ,2)^2 # cross-entropy loss with L2 regularization\n",
    "    ));\n",
    "\n",
    "    # train -\n",
    "    model = learn(X,y,model, maxiter = maxiter, verbose = true); # this is learning the model parameters\n",
    "\n",
    "    # return -\n",
    "    model;\n",
    "end;\n",
    "=#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fbd99c-9b57-430a-97c5-ebd4ce0bc468",
   "metadata": {},
   "source": [
    "Let's use the updated `model_logistic::MyLogisticRegressionClassificationModel` instance (with parameters learned from the `training` data) and test how well we can classify data in the `test` dataset.\n",
    "\n",
    "> __Inference__: We run the classification operation on the (unseen) test data [using the `classify(...)` method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/#VLDataScienceMachineLearningPackage.classify). This method takes a feature array `X` and the (trained) model instance. It returns the probability of a label in the `P::Array{Float64,2}` array (which is different than the Perceptron). Each row of `P` corresponds to a test instance, in which each column corresponds to a label, in the case `1` and `-1`.\n",
    "\n",
    "We store the actual (correct) label in the `y_logistic::Array{Int64,1}` vector. We compute the predicted label for each test instance by finding the highest probability column. We store the predicted labels in the `ŷ_logistic::Array{Int64,1}` vector.\n",
    "\n",
    "__Instructions:__ Uncomment the code below to compute logistic regression predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "045a7e5f-35fc-4e8c-8099-d77ee9fa9f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=\n",
    "ŷ_logistic,y_logistic, P = let\n",
    "\n",
    "    D = test; # What dataset are you going to use?\n",
    "    number_of_examples = size(D,1); # how many examples do we have (rows)\n",
    "    X = [D[:,1:end-1] ones(number_of_examples)]; # remove label and add bias term\n",
    "    y = D[:,end]; # actual labels\n",
    "    number_of_features = size(X,2); # how many features do we have (cols)?\n",
    "\n",
    "    # compute the estimated labels -\n",
    "    P = classify(X,model_logistic) # logistic regression returns a x x 2 array holding the probability\n",
    "\n",
    "    # convert the probability to a choice ... for each row (test instance), compute the col with the highest probability\n",
    "    ŷ = zeros(Int64, number_of_examples);\n",
    "    for i ∈ 1:number_of_examples\n",
    "        a = argmax(P[i,:]); # col index with largest value\n",
    "        ŷ[i] = 1; # default\n",
    "        if (a == 2)\n",
    "            ŷ[i] = -1;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # return -\n",
    "    ŷ, y, P\n",
    "end;\n",
    "=#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff583719",
   "metadata": {},
   "source": [
    "What does our probability array look like? Is the classification confident (i.e., probabilities close to 0 or 1) or uncertain (probabilities close to 0.5)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23f418a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=\n",
    "P\n",
    "=#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a34c17",
   "metadata": {},
   "source": [
    "__Visualize the misses__: Using the test dataset, we'll show (with gray circles) which samples the logistic regression classifier is unable to predict correctly.\n",
    "\n",
    "__Instructions:__ Uncomment the code below to visualize logistic regression misses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef62a4cd-9e1d-4806-9e41-0c7a13f7d584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=\n",
    "let\n",
    "\n",
    "    model = model_logistic; # which model am I using?\n",
    "    dataset = test; # what dataset am I looking at?\n",
    "    caselabel = \"test\";\n",
    "    actual = y_logistic;\n",
    "    predicted = ŷ_logistic;\n",
    "    number_of_points = size(dataset,1); # number of rows\n",
    "    p = plot(bg=\"gray95\", background_color_outside=\"white\", framestyle = :box, fg_legend = :transparent); # make an empty plot\n",
    "    \n",
    "    # plot label = 1\n",
    "    testlabel = 1;\n",
    "    i = findfirst(label -> label == testlabel,  dataset[:,3])\n",
    "    c = my_color_dictionary[testlabel]\n",
    "    scatter!([dataset[i,1]], [dataset[i,2]], label=\"Label: $(testlabel)\", c=c)\n",
    "\n",
    "    # plot label = -1\n",
    "    testlabel = -1;\n",
    "    i = findfirst(label -> label == testlabel,  dataset[:,3])\n",
    "    c = my_color_dictionary[testlabel]\n",
    "    scatter!([dataset[i,1]], [dataset[i,2]], label=\"Label: $(testlabel)\", c=c)\n",
    "\n",
    "    # let's draw the separating hyperplane (in our case, a line)\n",
    "    p = model.β;\n",
    "    number_of_plane_points = 200;\n",
    "    x₂ = zeros(number_of_plane_points);\n",
    "    x₁ = range(-1,stop=1,length = number_of_plane_points) |> collect;\n",
    "    for i ∈ 1:number_of_plane_points\n",
    "        x₂[i] = -1*((p[1]/p[2])*x₁[i] + p[3]/p[2]);\n",
    "    end\n",
    "    plot!(x₁,x₂,lw=2, c=:green, label=\"Learned boundary\")\n",
    "    \n",
    "    # data -\n",
    "    for i ∈ 1:number_of_points\n",
    "        actuallabel = actual[i]; # actual label\n",
    "        testlabel = predicted[i]; # predited label\n",
    "\n",
    "        c = :gray60;\n",
    "        if (actuallabel == testlabel)\n",
    "            c = my_color_dictionary[actuallabel]\n",
    "        end\n",
    "        scatter!([dataset[i, 1]], [dataset[i, 2]], label=\"\", mec=:navy, c=c)\n",
    "    end\n",
    "\n",
    "    title!(\"Logistic: $(caselabel)\", fontsize=18)\n",
    "    xlabel!(\"Feature 1 (AU)\", fontsize=18);\n",
    "    ylabel!(\"Feature 2 (AU)\", fontsize=18);\n",
    "end\n",
    "=#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2288e05",
   "metadata": {},
   "source": [
    "__Performance__: Once we have has converged (or exhasted our iterations), we can evaluate the binary classifier's performance using various metrics. The central idea is to compare the predicted labels $\\hat{y}_{i}$ to the actual labels $y_{i}$ in the `test` dataset and measure wins (when the label is the same) and losses (label is different). This is easily represented in [the confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
    "\n",
    "> __Error analysis using the confusion matrix__\n",
    ">\n",
    "> Total mistakes (or mistake percentage) is only part of the story. We should understand whether we are biased toward false positives or false negatives. How many times did we predict a banknote was forged when it was genuine (false positive)? How many times did we predict genuine when it was forged (false negative)? For this we use a confusion matrix. \n",
    ">\n",
    "> The confusion matrix for a binary classifier is typically structured as:\n",
    ">\n",
    ">|                     | **Predicted Positive** | **Predicted Negative** |\n",
    ">|---------------------|------------------------|------------------------|\n",
    ">| **Actual Positive** | True Positive (TP)     | False Negative (FN)    |\n",
    ">| **Actual Negative** | False Positive (FP)    | True Negative (TN)     |\n",
    "\n",
    "We compute the __confusion matrix__ to get these counts. We use the [confusion(...) method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/#VLDataScienceMachineLearningPackage.confusion), which takes actual labels and estimated labels, returning the confusion matrix.\n",
    "\n",
    "__Instructions:__ Uncomment the code below to compute the logistic confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf9b194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=\n",
    "CM_logistic = confusion(y_logistic, ŷ_logistic)\n",
    "=#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73984886",
   "metadata": {},
   "source": [
    "Let's compute the overall error rate for the logistic regression using [the confusion matrix](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/#VLDataScienceMachineLearningPackage.confusion)\n",
    "\n",
    "__Instructions:__ Uncomment the code below to compute the logistic regression accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bccf0839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=\n",
    "number_of_test_points = length(y_perceptron);\n",
    "correct_prediction_logistic = CM_logistic[1,1] + CM_logistic[2,2];\n",
    "(correct_prediction_logistic/number_of_test_points) |> f-> println(\"Fraction correct: $(f) Fraction incorrect $(1-f)\")\n",
    "=#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f364e6cb-20fc-44ab-aa05-05abde70e3d3",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cb60f0",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This lab demonstrates that linear classifiers like Perceptron and Logistic Regression cannot perfectly classify non-linearly separable datasets such as XOR patterns, regardless of training iterations or parameter optimization.\n",
    "\n",
    "> __Key Takeaways:__\n",
    ">\n",
    "> * **Linear separability determines classifier success:** Perceptron and Logistic Regression achieve high accuracy on linearly separable data but fail systematically on non-linearly separable patterns like XOR, where no single linear boundary can separate the classes.\n",
    "> * **Both linear methods share the same fundamental limitation:** Despite different learning algorithms (online learning for Perceptron versus gradient descent for Logistic Regression), both models learn similar decision boundaries and produce comparable error rates on non-separable data because they are constrained to linear separation.\n",
    "> * **Confusion matrices quantify the separability problem:** Performance metrics computed from confusion matrices reveal that classification accuracy drops significantly when data violates linear separability assumptions, confirming that these failures are fundamental rather than implementation issues.\n",
    "\n",
    "\n",
    "The XOR problem reveals why linear classifiers fail systematically on certain datasets and motivates the need for models with non-linear decision boundaries, such as multi-layer neural networks. \n",
    "\n",
    "___\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12.4",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
