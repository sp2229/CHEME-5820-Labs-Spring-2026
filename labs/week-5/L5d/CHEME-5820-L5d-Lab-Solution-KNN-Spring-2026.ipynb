{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a39ab0",
   "metadata": {},
   "source": [
    "# L5d: K-Nearest Neighbors (KNN) classification of a QSAR Chemical Compound Dataset\n",
    "In this lesson, we will perform a K-Nearest Neighbors (KNN) analysis on a QSAR (Quantitative Structure-Activity Relationship) chemical compound dataset, where the goal is to predict the label (biodegradability) of chemical compounds based on their structural features.\n",
    "\n",
    "> __Learning Objectives:__\n",
    "> \n",
    "> By the end of this lab, you will be able to:\n",
    "> * __Prepare KNN-ready data:__ Split the QSAR dataset into training and test partitions and convert each example into a feature-label tuple. Re-encode the original class labels into the binary targets used by the KNN classifier.\n",
    "> * __Build and sanity-check a kernel KNN classifier:__ Define an RBF kernel and build a weighted kernelized KNN classification model from the training data. Verify the training kernel matrix is positive semidefinite using an eigenvalue check.\n",
    "> * __Evaluate and tune model performance:__ Compute accuracy, precision, and recall from the confusion matrix on the test set. Compare these metrics across neighbor-count and kernel-width settings during the hyperparameter sweep.\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0facab",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "First, we set up the computational environment by including the `Include.jl` file and loading any needed resources.\n",
    "\n",
    "> __Environment Setup with Include.jl__\n",
    ">\n",
    "> The [`include(...)` command](https://docs.julialang.org/en/v1/base/base/#include) evaluates the contents of the input source file, `Include.jl`, in the notebook's global scope. The `Include.jl` file sets paths, loads required external packages, etc. For additional information on functions and types used in this material, see the [Julia programming language documentation](https://docs.julialang.org/en/v1/).\n",
    "\n",
    "Let's set up our code environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c652473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(joinpath(@__DIR__, \"Include.jl\")); # include the Include.jl file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5407682",
   "metadata": {},
   "source": [
    "In addition to standard Julia libraries, we'll also use [the `VLDataScienceMachineLearningPackage.jl` package](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl). Check out [the documentation](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/) for more information on the functions, types, and data used in this material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d4be2f",
   "metadata": {},
   "source": [
    "### Data\n",
    "The dataset we will be using is a QSAR chemical compound dataset, which contains structural features of chemical compounds along with their biodegradability labels. The dataset is available here:\n",
    "\n",
    "* Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., & Consonni, V. (2013). QSAR biodegradation Dataset. UCI Machine Learning Repository. https://doi.org/10.24432/C5H60M.\n",
    "\n",
    "We built [the `MyQSARBiodegradationDataset()` helper function](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/data/#VLDataScienceMachineLearningPackage.MyQSARBiodegradationDataset) to load the dataset, which is exported by [the `VLDataScienceMachineLearningPackage.jl` package](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl). The data is returned as a `DataFrame` and stored in the `originaldataset` variable. \n",
    "\n",
    "There are 1055 rows (chemical compounds) and 42 columns (features and labels) in the dataset. The features include various structural descriptors of the chemical compounds, and the label indicates whether the compound is biodegradable (`1`) or not (`2`); see [the openML documentation](https://www.openml.org/search?type=data&id=1494&sort=runs&status=active) for more information on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77f43630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>1055Ã—42 DataFrame</span></div><div style = \"float: right; font-style: italic;\"><span>1030 rows omitted</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"columnLabelRow\"><th class = \"stubheadLabel\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">V1</th><th style = \"text-align: left;\">V2</th><th style = \"text-align: left;\">V3</th><th style = \"text-align: left;\">V4</th><th style = \"text-align: left;\">V5</th><th style = \"text-align: left;\">V6</th><th style = \"text-align: left;\">V7</th><th style = \"text-align: left;\">V8</th><th style = \"text-align: left;\">V9</th><th style = \"text-align: left;\">V10</th><th style = \"text-align: left;\">V11</th><th style = \"text-align: left;\">V12</th><th style = \"text-align: left;\">V13</th><th style = \"text-align: left;\">V14</th><th style = \"text-align: left;\">V15</th><th style = \"text-align: left;\">V16</th><th style = \"text-align: left;\">V17</th><th style = \"text-align: left;\">V18</th><th style = \"text-align: left;\">V19</th><th style = \"text-align: left;\">V20</th><th style = \"text-align: left;\">V21</th><th style = \"text-align: left;\">V22</th><th style = \"text-align: left;\">V23</th><th style = \"text-align: left;\">V24</th><th style = \"text-align: left;\">V25</th><th style = \"text-align: left;\">V26</th><th style = \"text-align: left;\">V27</th><th style = \"text-align: left;\">V28</th><th style = \"text-align: left;\">V29</th><th style = \"text-align: left;\">V30</th><th style = \"text-align: left;\">V31</th><th style = \"text-align: left;\">V32</th><th style = \"text-align: left;\">V33</th><th style = \"text-align: left;\">V34</th><th style = \"text-align: left;\">V35</th><th style = \"text-align: left;\">V36</th><th style = \"text-align: left;\">V37</th><th style = \"text-align: left;\">V38</th><th style = \"text-align: left;\">V39</th><th style = \"text-align: left;\">V40</th><th style = \"text-align: left;\">V41</th><th style = \"text-align: left;\">Class</th></tr><tr class = \"columnLabelRow\"><th class = \"stubheadLabel\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th></tr></thead><tbody><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">3.919</td><td style = \"text-align: right;\">2.6909</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">31.4</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">3.106</td><td style = \"text-align: right;\">2.55</td><td style = \"text-align: right;\">9.002</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.96</td><td style = \"text-align: right;\">1.142</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.201</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.932</td><td style = \"text-align: right;\">0.011</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">4.489</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2.949</td><td style = \"text-align: right;\">1.591</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">7.253</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">4.17</td><td style = \"text-align: right;\">2.1144</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">30.8</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">2.461</td><td style = \"text-align: right;\">1.393</td><td style = \"text-align: right;\">8.723</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0.989</td><td style = \"text-align: right;\">1.144</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.104</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2.214</td><td style = \"text-align: right;\">-0.204</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">1.542</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">3.315</td><td style = \"text-align: right;\">1.967</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">7.257</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">3.932</td><td style = \"text-align: right;\">3.2512</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">26.7</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">3.279</td><td style = \"text-align: right;\">2.585</td><td style = \"text-align: right;\">9.11</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.009</td><td style = \"text-align: right;\">1.152</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.092</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.942</td><td style = \"text-align: right;\">-0.008</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">4.891</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3.076</td><td style = \"text-align: right;\">2.417</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">7.601</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">3.0</td><td style = \"text-align: right;\">2.7098</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">20.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">2.1</td><td style = \"text-align: right;\">0.918</td><td style = \"text-align: right;\">6.594</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.108</td><td style = \"text-align: right;\">1.167</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.024</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.414</td><td style = \"text-align: right;\">1.073</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">8.361</td><td style = \"text-align: right;\">1.333</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3.046</td><td style = \"text-align: right;\">5.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">6.69</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">4.236</td><td style = \"text-align: right;\">3.3944</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">29.4</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">-0.271</td><td style = \"text-align: right;\">3.449</td><td style = \"text-align: right;\">2.753</td><td style = \"text-align: right;\">9.528</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1.004</td><td style = \"text-align: right;\">1.147</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.137</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.985</td><td style = \"text-align: right;\">-0.002</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">10.348</td><td style = \"text-align: right;\">5.588</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">3.351</td><td style = \"text-align: right;\">2.405</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">8.003</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: right;\">4.236</td><td style = \"text-align: right;\">3.4286</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">28.6</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">-0.275</td><td style = \"text-align: right;\">3.313</td><td style = \"text-align: right;\">2.522</td><td style = \"text-align: right;\">9.383</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1.014</td><td style = \"text-align: right;\">1.149</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.119</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.98</td><td style = \"text-align: right;\">-0.008</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">10.276</td><td style = \"text-align: right;\">4.746</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">3.351</td><td style = \"text-align: right;\">2.556</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">7.904</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: right;\">5.0</td><td style = \"text-align: right;\">5.0476</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">11.1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">2.872</td><td style = \"text-align: right;\">0.722</td><td style = \"text-align: right;\">9.657</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.092</td><td style = \"text-align: right;\">1.153</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.125</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2.0</td><td style = \"text-align: right;\">0.446</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">18.375</td><td style = \"text-align: right;\">0.8</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">4.712</td><td style = \"text-align: right;\">4.583</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">9.303</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: right;\">4.525</td><td style = \"text-align: right;\">3.8301</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">31.6</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">-0.039</td><td style = \"text-align: right;\">3.418</td><td style = \"text-align: right;\">2.468</td><td style = \"text-align: right;\">9.786</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">0.98</td><td style = \"text-align: right;\">1.142</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.179</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2.119</td><td style = \"text-align: right;\">-0.002</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">11.115</td><td style = \"text-align: right;\">3.889</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">3.379</td><td style = \"text-align: right;\">2.143</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">7.95</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: right;\">4.596</td><td style = \"text-align: right;\">3.0777</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">44.4</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">2.97</td><td style = \"text-align: right;\">0.875</td><td style = \"text-align: right;\">9.54</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.968</td><td style = \"text-align: right;\">1.115</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.328</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2.175</td><td style = \"text-align: right;\">0.041</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">1.069</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">3.626</td><td style = \"text-align: right;\">1.917</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">7.939</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: right;\">5.04</td><td style = \"text-align: right;\">3.6112</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">41.2</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">-1.29</td><td style = \"text-align: right;\">3.483</td><td style = \"text-align: right;\">1.258</td><td style = \"text-align: right;\">10.159</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">1.069</td><td style = \"text-align: right;\">1.127</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.199</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2.323</td><td style = \"text-align: right;\">0.005</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">30.959</td><td style = \"text-align: right;\">1.711</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3.888</td><td style = \"text-align: right;\">3.5</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">8.706</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: right;\">4.91</td><td style = \"text-align: right;\">2.7414</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">52.9</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">-0.302</td><td style = \"text-align: right;\">3.364</td><td style = \"text-align: right;\">0.629</td><td style = \"text-align: right;\">10.06</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">1.018</td><td style = \"text-align: right;\">1.098</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.279</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2.333</td><td style = \"text-align: right;\">0.004</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">10.717</td><td style = \"text-align: right;\">1.207</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">3.767</td><td style = \"text-align: right;\">2.5</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">8.427</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: right;\">3.618</td><td style = \"text-align: right;\">2.1906</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">29.4</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">2.333</td><td style = \"text-align: right;\">1.522</td><td style = \"text-align: right;\">7.853</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.959</td><td style = \"text-align: right;\">1.147</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.178</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.732</td><td style = \"text-align: right;\">0.294</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">2.094</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2.794</td><td style = \"text-align: right;\">1.7</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">6.333</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: right;\">4.214</td><td style = \"text-align: right;\">2.6272</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">30.0</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">2.523</td><td style = \"text-align: right;\">1.459</td><td style = \"text-align: right;\">8.755</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.959</td><td style = \"text-align: right;\">1.145</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.213</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.902</td><td style = \"text-align: right;\">-0.181</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">2.052</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2.998</td><td style = \"text-align: right;\">1.722</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">6.77</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2</td></tr><tr><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1044</td><td style = \"text-align: right;\">5.399</td><td style = \"text-align: right;\">3.4164</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">38.7</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">3.424</td><td style = \"text-align: right;\">1.084</td><td style = \"text-align: right;\">10.644</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">0.977</td><td style = \"text-align: right;\">1.127</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.271</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2.403</td><td style = \"text-align: right;\">-0.002</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">1.65</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3.818</td><td style = \"text-align: right;\">2.147</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">8.453</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1045</td><td style = \"text-align: right;\">4.807</td><td style = \"text-align: right;\">3.3179</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">43.8</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">3.297</td><td style = \"text-align: right;\">0.881</td><td style = \"text-align: right;\">9.833</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1.012</td><td style = \"text-align: right;\">1.113</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.298</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2.246</td><td style = \"text-align: right;\">-0.025</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">1.06</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3.926</td><td style = \"text-align: right;\">2.568</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">8.562</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1046</td><td style = \"text-align: right;\">4.607</td><td style = \"text-align: right;\">3.0008</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">41.2</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">3.256</td><td style = \"text-align: right;\">1.187</td><td style = \"text-align: right;\">9.631</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">0.996</td><td style = \"text-align: right;\">1.105</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.302</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2.194</td><td style = \"text-align: right;\">-0.025</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">1.544</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3.865</td><td style = \"text-align: right;\">2.352</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">8.506</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1047</td><td style = \"text-align: right;\">5.313</td><td style = \"text-align: right;\">2.7782</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">40.0</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">3.564</td><td style = \"text-align: right;\">1.697</td><td style = \"text-align: right;\">10.592</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">0.976</td><td style = \"text-align: right;\">1.125</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.274</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2.329</td><td style = \"text-align: right;\">-0.001</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">10.535</td><td style = \"text-align: right;\">2.986</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">3.692</td><td style = \"text-align: right;\">2.161</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">8.482</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1048</td><td style = \"text-align: right;\">5.103</td><td style = \"text-align: right;\">3.9184</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">35.0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">3.649</td><td style = \"text-align: right;\">1.197</td><td style = \"text-align: right;\">10.456</td><td style = \"text-align: right;\">11</td><td style = \"text-align: right;\">1.08</td><td style = \"text-align: right;\">1.144</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.195</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2.38</td><td style = \"text-align: right;\">0.001</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">41.425</td><td style = \"text-align: right;\">1.74</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3.967</td><td style = \"text-align: right;\">3.619</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">8.984</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1049</td><td style = \"text-align: right;\">5.265</td><td style = \"text-align: right;\">3.3444</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">6</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">35.3</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">0.134</td><td style = \"text-align: right;\">3.998</td><td style = \"text-align: right;\">2.353</td><td style = \"text-align: right;\">10.632</td><td style = \"text-align: right;\">6</td><td style = \"text-align: right;\">1.012</td><td style = \"text-align: right;\">1.123</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.296</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2.3</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">3.88</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">4.698</td><td style = \"text-align: right;\">2.463</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">9.666</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1050</td><td style = \"text-align: right;\">5.029</td><td style = \"text-align: right;\">2.5966</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">46.7</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">-0.761</td><td style = \"text-align: right;\">3.837</td><td style = \"text-align: right;\">1.418</td><td style = \"text-align: right;\">10.634</td><td style = \"text-align: right;\">10</td><td style = \"text-align: right;\">1.02</td><td style = \"text-align: right;\">1.111</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.254</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2.403</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">22.765</td><td style = \"text-align: right;\">2.586</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">3.919</td><td style = \"text-align: right;\">2.611</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">8.958</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1051</td><td style = \"text-align: right;\">5.431</td><td style = \"text-align: right;\">2.8955</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">32.1</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0.374</td><td style = \"text-align: right;\">3.233</td><td style = \"text-align: right;\">0.832</td><td style = \"text-align: right;\">10.681</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">0.982</td><td style = \"text-align: right;\">1.144</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.232</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2.394</td><td style = \"text-align: right;\">-0.007</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">11.254</td><td style = \"text-align: right;\">1.055</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">6</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">3.573</td><td style = \"text-align: right;\">2.242</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">8.088</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1052</td><td style = \"text-align: right;\">5.287</td><td style = \"text-align: right;\">3.3732</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">9</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">35.3</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">9</td><td style = \"text-align: right;\">9</td><td style = \"text-align: right;\">-5.256</td><td style = \"text-align: right;\">4.319</td><td style = \"text-align: right;\">2.346</td><td style = \"text-align: right;\">11.029</td><td style = \"text-align: right;\">21</td><td style = \"text-align: right;\">1.043</td><td style = \"text-align: right;\">1.14</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.178</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2.462</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">71.167</td><td style = \"text-align: right;\">3.396</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">3.787</td><td style = \"text-align: right;\">3.083</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">9.278</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1053</td><td style = \"text-align: right;\">4.869</td><td style = \"text-align: right;\">1.767</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">9</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">44.4</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">14</td><td style = \"text-align: right;\">-0.391</td><td style = \"text-align: right;\">4.435</td><td style = \"text-align: right;\">1.073</td><td style = \"text-align: right;\">11.072</td><td style = \"text-align: right;\">9</td><td style = \"text-align: right;\">1.016</td><td style = \"text-align: right;\">1.123</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.261</td><td style = \"text-align: right;\">3</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2.314</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">33.54</td><td style = \"text-align: right;\">6.465</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">13</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">3.848</td><td style = \"text-align: right;\">2.576</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">9.537</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1054</td><td style = \"text-align: right;\">5.158</td><td style = \"text-align: right;\">1.6914</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">36</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">9</td><td style = \"text-align: right;\">56.1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">44</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">4.902</td><td style = \"text-align: right;\">0.257</td><td style = \"text-align: right;\">11.817</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.007</td><td style = \"text-align: right;\">1.093</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.41</td><td style = \"text-align: right;\">147</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">2.622</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">1.535</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">16</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">5.808</td><td style = \"text-align: right;\">2.055</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">11.055</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1055</td><td style = \"text-align: right;\">5.076</td><td style = \"text-align: right;\">2.6588</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">54.5</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">3.792</td><td style = \"text-align: right;\">0.673</td><td style = \"text-align: right;\">10.327</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.003</td><td style = \"text-align: right;\">1.089</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1.364</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2.363</td><td style = \"text-align: right;\">-0.001</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">2.106</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">4.009</td><td style = \"text-align: right;\">2.206</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">9.13</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">2</td><td style = \"text-align: right;\">1</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccccccc}\n",
       "\t& V1 & V2 & V3 & V4 & V5 & V6 & V7 & V8 & V9 & V10 & V11 & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Int64 & Int64 & Int64 & Int64 & Int64 & Float64 & Int64 & Int64 & Int64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 3.919 & 2.6909 & 0 & 0 & 0 & 0 & 0 & 31.4 & 2 & 0 & 0 & $\\dots$ \\\\\n",
       "\t2 & 4.17 & 2.1144 & 0 & 0 & 0 & 0 & 0 & 30.8 & 1 & 1 & 0 & $\\dots$ \\\\\n",
       "\t3 & 3.932 & 3.2512 & 0 & 0 & 0 & 0 & 0 & 26.7 & 2 & 4 & 0 & $\\dots$ \\\\\n",
       "\t4 & 3.0 & 2.7098 & 0 & 0 & 0 & 0 & 0 & 20.0 & 0 & 2 & 0 & $\\dots$ \\\\\n",
       "\t5 & 4.236 & 3.3944 & 0 & 0 & 0 & 0 & 0 & 29.4 & 2 & 4 & 0 & $\\dots$ \\\\\n",
       "\t6 & 4.236 & 3.4286 & 0 & 0 & 0 & 0 & 0 & 28.6 & 2 & 4 & 0 & $\\dots$ \\\\\n",
       "\t7 & 5.0 & 5.0476 & 1 & 0 & 0 & 0 & 0 & 11.1 & 0 & 3 & 0 & $\\dots$ \\\\\n",
       "\t8 & 4.525 & 3.8301 & 0 & 0 & 0 & 0 & 0 & 31.6 & 3 & 2 & 0 & $\\dots$ \\\\\n",
       "\t9 & 4.596 & 3.0777 & 0 & 0 & 0 & 0 & 2 & 44.4 & 2 & 0 & 0 & $\\dots$ \\\\\n",
       "\t10 & 5.04 & 3.6112 & 0 & 0 & 1 & 0 & 2 & 41.2 & 0 & 4 & 3 & $\\dots$ \\\\\n",
       "\t11 & 4.91 & 2.7414 & 0 & 0 & 0 & 0 & 2 & 52.9 & 0 & 2 & 0 & $\\dots$ \\\\\n",
       "\t12 & 3.618 & 2.1906 & 0 & 0 & 0 & 0 & 0 & 29.4 & 2 & 0 & 0 & $\\dots$ \\\\\n",
       "\t13 & 4.214 & 2.6272 & 0 & 0 & 0 & 0 & 0 & 30.0 & 3 & 0 & 0 & $\\dots$ \\\\\n",
       "\t14 & 3.732 & 2.3391 & 0 & 0 & 0 & 0 & 0 & 30.0 & 2 & 0 & 0 & $\\dots$ \\\\\n",
       "\t15 & 3.879 & 2.5951 & 0 & 0 & 0 & 0 & 0 & 31.0 & 2 & 0 & 0 & $\\dots$ \\\\\n",
       "\t16 & 3.942 & 2.7719 & 1 & 0 & 0 & 0 & 0 & 31.6 & 2 & 0 & 0 & $\\dots$ \\\\\n",
       "\t17 & 3.966 & 2.852 & 1 & 0 & 0 & 0 & 0 & 32.0 & 2 & 0 & 0 & $\\dots$ \\\\\n",
       "\t18 & 3.732 & 2.3761 & 0 & 0 & 1 & 0 & 0 & 26.3 & 2 & 0 & 1 & $\\dots$ \\\\\n",
       "\t19 & 4.0 & 2.6264 & 0 & 0 & 0 & 0 & 0 & 23.1 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t20 & 2.0 & 1.1521 & 0 & 0 & 0 & 0 & 0 & 16.7 & 1 & 1 & 0 & $\\dots$ \\\\\n",
       "\t21 & 3.732 & 2.4062 & 0 & 0 & 0 & 0 & 0 & 27.8 & 2 & 1 & 0 & $\\dots$ \\\\\n",
       "\t22 & 4.0 & 2.3699 & 0 & 0 & 0 & 0 & 0 & 30.0 & 1 & 1 & 0 & $\\dots$ \\\\\n",
       "\t23 & 3.414 & 2.2525 & 0 & 0 & 0 & 0 & 0 & 20.0 & 2 & 2 & 0 & $\\dots$ \\\\\n",
       "\t24 & 4.17 & 2.8042 & 0 & 0 & 0 & 0 & 0 & 23.1 & 2 & 2 & 0 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m1055Ã—42 DataFrame\u001b[0m\n",
       "\u001b[1m  Row \u001b[0mâ”‚\u001b[1m V1      \u001b[0m\u001b[1m V2      \u001b[0m\u001b[1m V3    \u001b[0m\u001b[1m V4    \u001b[0m\u001b[1m V5    \u001b[0m\u001b[1m V6    \u001b[0m\u001b[1m V7    \u001b[0m\u001b[1m V8      \u001b[0m\u001b[1m V9    \u001b[0m\u001b[1m V\u001b[0m â‹¯\n",
       "\u001b[1m      \u001b[0mâ”‚\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Int64 \u001b[0m\u001b[90m Int64 \u001b[0m\u001b[90m Int64 \u001b[0m\u001b[90m Int64 \u001b[0m\u001b[90m Int64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Int64 \u001b[0m\u001b[90m I\u001b[0m â‹¯\n",
       "â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
       "    1 â”‚   3.919   2.6909      0      0      0      0      0     31.4      2    â‹¯\n",
       "    2 â”‚   4.17    2.1144      0      0      0      0      0     30.8      1\n",
       "    3 â”‚   3.932   3.2512      0      0      0      0      0     26.7      2\n",
       "    4 â”‚   3.0     2.7098      0      0      0      0      0     20.0      0\n",
       "    5 â”‚   4.236   3.3944      0      0      0      0      0     29.4      2    â‹¯\n",
       "    6 â”‚   4.236   3.4286      0      0      0      0      0     28.6      2\n",
       "    7 â”‚   5.0     5.0476      1      0      0      0      0     11.1      0\n",
       "    8 â”‚   4.525   3.8301      0      0      0      0      0     31.6      3\n",
       "  â‹®   â”‚    â‹®        â‹®       â‹®      â‹®      â‹®      â‹®      â‹®       â‹®       â‹®      â‹±\n",
       " 1049 â”‚   5.265   3.3444      2      0      6      0      1     35.3      2    â‹¯\n",
       " 1050 â”‚   5.029   2.5966      0      0      0      0      4     46.7      0\n",
       " 1051 â”‚   5.431   2.8955      0      0      0      2      0     32.1      4\n",
       " 1052 â”‚   5.287   3.3732      0      0      9      0      0     35.3      0\n",
       " 1053 â”‚   4.869   1.767       0      1      9      0      5     44.4      0    â‹¯\n",
       " 1054 â”‚   5.158   1.6914      2      0     36      0      9     56.1      0\n",
       " 1055 â”‚   5.076   2.6588      2      0      0      0      4     54.5      0\n",
       "\u001b[36m                                                33 columns and 1040 rows omitted\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "originaldataset = MyQSARBiodegradationDataset() # load the dataset using the helper function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171bee10",
   "metadata": {},
   "source": [
    "## Task 1: Let's Split the Data into Training and Test Sets\n",
    "In this task, we will split the dataset into training and test sets. The distinction between training and test for a KNN classifier is a little different than for other types of classifiers, because KNN is a memory-based learning algorithm. Thus, KNN does not learn a parametric model from the training data; instead, it simply stores the training data and makes predictions based on the similarity of new instances (from the test dataset) to the stored training instances.\n",
    "\n",
    "> __What is going on in this code block?__\n",
    ">\n",
    "> We shuffle row indices, split the dataset into training and test subsets, convert each row into an `(x, y)` named tuple, and map labels from `1/2` to `1/-1`. This representation matches the classifier interface and keeps features and labels together for each example.\n",
    "\n",
    "Let's save the training and test datasets in the `training::Array{NamedTuple,1}` and `test::Array{NamedTuple,1}` variables, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdc686c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test = let\n",
    "\n",
    "    # initialize -\n",
    "    total_number_of_examples = nrow(originaldataset); \n",
    "    number_of_training_examples = 844; # approximately 80% of the data\n",
    "    number_of_test_examples = total_number_of_examples - number_of_training_examples; # approximately 20% of the data\n",
    "\n",
    "    # generate a random permutation of the row indices of the dataset\n",
    "    shuffled_indices = randperm(total_number_of_examples);\n",
    "    \n",
    "    # split the shuffled indices into training and test indices\n",
    "    training_indices = shuffled_indices[1:number_of_training_examples];\n",
    "    test_indices = shuffled_indices[number_of_training_examples+1:end];\n",
    "\n",
    "    # get the training and test datasets using the training and test indices\n",
    "    training_dataset = originaldataset[training_indices, :];\n",
    "    test_dataset = originaldataset[test_indices, :];\n",
    "    \n",
    "    # make the training into arrays of named tuples\n",
    "    training = Array{NamedTuple,1}(undef, number_of_training_examples);\n",
    "    for i âˆˆ 1:number_of_training_examples\n",
    "        features = training_dataset[i, 1:end-1] |> values |> collect .|> Float64; # all columns except the last one are features\n",
    "        label = training_dataset[i, end] == 1 ? 1 : -1; # the last column is the label\n",
    "        training[i] = (x = features, y = label); # create a named tuple with the features and label\n",
    "    end\n",
    "\n",
    "    # make the test into arrays of named tuples\n",
    "    test = Array{NamedTuple,1}(undef, number_of_test_examples);\n",
    "    for i âˆˆ 1:number_of_test_examples\n",
    "        features = test_dataset[i, 1:end-1] |> values |> collect .|> Float64; # all columns except the last one are features\n",
    "        label = test_dataset[i, end] == 1 ? 1 : -1; # the last column is the label\n",
    "        test[i] = (x = features, y = label); # create a named tuple with the features and label\n",
    "    end\n",
    "    \n",
    "    (training, test); # return the training and test datasets as arrays of named tuples\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171a0e47",
   "metadata": {},
   "source": [
    "__Are the labels in the training dataset balanced?__ Since we are using a KNN classifier, it is important to have balanced labels in the training dataset. If the labels are imbalanced in our reference dataset, the KNN classifier may be biased towards the majority class, which can lead to poor performance on the minority class. We can check for label balance by looking at the distribution of labels in the training datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ac4d56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of positive labels in the training dataset: 0.6611374407582938\n",
      "Fraction of negative labels in the training dataset: 0.33886255924170616\n"
     ]
    }
   ],
   "source": [
    "let \n",
    "\n",
    "    # initialize -\n",
    "    D = training; # specify what data set we are working with\n",
    "    number_of_training_examples = length(D); # how many examples are in the training dataset?\n",
    "\n",
    "    # Fancy! Let's use a list comprehension to count the number of positive and negative labels in the training dataset. The `sum()` function will sum up the number of times the condition is true for each example in the training dataset.\n",
    "    count_positive_labels = sum(D[i].y == 1 for i âˆˆ 1:number_of_training_examples); # how many positive labels are in the training dataset?\n",
    "    count_negative_labels = sum(D[i].y == -1 for i âˆˆ 1:number_of_training_examples); # how many negative labels are in the training dataset?\n",
    "    \n",
    "    # print the results (fraction of positive and negative labels in the training dataset)\n",
    "    println(\"Fraction of positive labels in the training dataset: \", count_positive_labels / number_of_training_examples);\n",
    "    println(\"Fraction of negative labels in the training dataset: \", count_negative_labels / number_of_training_examples);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccebb4dc",
   "metadata": {},
   "source": [
    "__Is this split representative of the overall dataset?__ We can check this by looking at the distribution of labels in the original dataset, and comparing them to the distribution in the training dataset. If the distributions are similar, then we can say that the split is representative of the overall dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b07fdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of positive labels in the original dataset: 0.6625592417061611\n",
      "Fraction of negative labels in the original dataset: 0.33744075829383885\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "\n",
    "    # initialize -\n",
    "    D = originaldataset; # specify what data set we are working with\n",
    "    number_of_examples = nrow(D); # how many examples are in the training dataset?\n",
    "\n",
    "    # Let's use a list comprehension to count the number of positive and negative labels in the original dataset. The `sum()` function will sum up the number of times the condition is true for each example in the original dataset.\n",
    "    count_positive_labels = sum(D[i, end] == 1 for i âˆˆ 1:number_of_examples); # how many positive labels are in the original dataset?\n",
    "    count_negative_labels = sum(D[i, end] == 2 for i âˆˆ 1:number_of_examples); # how many negative labels are in the original dataset?\n",
    "    \n",
    "    # print the results (fraction of positive and negative labels in the original dataset)\n",
    "    println(\"Fraction of positive labels in the original dataset: \", count_positive_labels / number_of_examples);\n",
    "    println(\"Fraction of negative labels in the original dataset: \", count_negative_labels / number_of_examples);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e13ee4a",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77345061",
   "metadata": {},
   "source": [
    "## Task 2: Let's look at our KNN Classifier\n",
    "In this task, we will build and evaluate the KNN classifier that we will be using to make predictions on the test dataset. We'll build [a `MyWeightedKernelizedKNNClassificationModel` model](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/#VLDataScienceMachineLearningPackage.MyWeightedKernelizedKNNClassificationModel) using the `training` dataset using the training dataset as the reference data.\n",
    "\n",
    "Let's build a kernel function $k:\\mathbb{R}^{m}\\times\\mathbb{R}^{m}\\to\\mathbb{R}$ to measure similarity. For now, let's make up our own kernel function, and save this function in the `k(x,y)::Function` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55812d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "k (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k(x,y,Î³) = exp(-Î³ * norm(x-y,2)^2) # RBF kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59173737",
   "metadata": {},
   "source": [
    "#### Check: Are we using a valid Kernel function?\n",
    "Let's check to see if the distance (similarity) metric we built is a valid kernel function.\n",
    "\n",
    "> __Condition:__\n",
    ">\n",
    "> A function $k:\\mathbb{R}^{m}\\times\\mathbb{R}^{m}\\to\\mathbb{R}$ is a _valid kernel function_ if and only if the kernel matrix $\\mathbf{K}\\in\\mathbb{R}^{n\\times{n}}$ is positive (semi)definite for all possible choices of the data vectors $\\mathbf{v}_i$, where $K_{ij} = k(\\mathbf{v}_i, \\mathbf{v}_j)$. If $\\mathbf{K}$ is positive (semi)definite, then for any real-valued vector $\\mathbf{x} \\in \\mathbb{R}^n$, the kernel matrix $\\mathbf{K}$ must satisfy $\\mathbf{x}^{\\top}\\mathbf{K}\\mathbf{x} \\geq 0$. \n",
    "\n",
    "Let's compute the kernel matrix `K::Array{Float64,2}` for a data matrix `X::Array{Float64,2}` using the distance/kernel function `k(x,y)::Function` we built above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4de8c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = let\n",
    "\n",
    "    D = training; # specify what data set we are working with\n",
    "    number_of_training_examples = size(D,1);\n",
    "    number_of_features = D[1].x |> length; # number of features in the dataset\n",
    "    Î³ = 0.5; # kernel parameter\n",
    "\n",
    "    # fill up the feature matrix -\n",
    "    X = zeros(number_of_training_examples, number_of_features); # initialize a matrix to hold the features\n",
    "    for i âˆˆ 1:number_of_training_examples\n",
    "        X[i,:] = D[i].x; # fill the matrix with the features from the training dataset\n",
    "    end\n",
    "\n",
    "    # fill up the kernel matrix -\n",
    "    K = zeros(number_of_training_examples,number_of_training_examples);\n",
    "    for i âˆˆ 1:number_of_training_examples\n",
    "        váµ¢ = X[i,:];\n",
    "        for j âˆˆ 1:number_of_training_examples\n",
    "            vâ±¼ = X[j,:];\n",
    "            K[i,j] = k(váµ¢,vâ±¼,Î³) # compute kernel value\n",
    "        end\n",
    "    end\n",
    "    K\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e342c5c3",
   "metadata": {},
   "source": [
    "Next, let's check to see if the kernel matrix `K::Array{Float64,2}` is positive (semi)definite by checking if all of its eigenvalues are non-negative.\n",
    "\n",
    "> __Check:__\n",
    ">\n",
    "> For this kernel to be valid, the kernel matrix $\\mathbf{K}$ needs to be positive (semi)definite, i.e., all eigenvalues $\\lambda_i \\geq 0$. We compute the eigenvalues using [`eigvals`](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.eigvals) and verify they are all non-negative using [the `@assert` macro](https://docs.julialang.org/en/v1/base/base/#Base.@assert) in combination with [the `all` function](https://docs.julialang.org/en/v1/base/collections/#Base.all-Tuple%7BAny%7D).\n",
    "\n",
    "Do we blow up? If not, the matrix is PSD for this dataset, which supports using this kernel in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33d97bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    Î» = eigvals(K);\n",
    "    @assert all(Î» .â‰¥ -1e-10) \"Kernel matrix is not PSD: min eigenvalue = $(minimum(Î»))\"\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9340ebf",
   "metadata": {},
   "source": [
    "Ok, so now let's build the KNN classifier model. There are a few design choices to make, such as the number of neighbors to consider (`K`), the kernel function, and any kernel parameters. \n",
    "\n",
    "> __How do we choose the `K` parameter?__\n",
    ">\n",
    "> The choice of `K` can significantly affect KNN performance. A common approach is to use cross-validation to select the optimal value (we will do this in the problem set). For this lab, we choose $K = mC + 1$, where $m > 0$ is an adjustable parameter and $C$ is the number of classes in the training dataset. We cam vary `m` to see how it affects the bias-variance tradeoff of the KNN classifier.\n",
    ">\n",
    "> In this context, __bias__ means a systematic error from an oversimplified neighborhood vote, and __variance__ means sensitivity to which training points happen to be in the dataset.\n",
    "> * __Small `K` values:__ The prediction is based on only a few nearby points, so the decision boundary can follow local structure closely. This usually reduces bias, but one noisy or mislabeled neighbor can change the vote, which increases variance.\n",
    "> * __Large `K` values:__ The prediction averages over many neighbors, so single noisy points have less influence on the vote. This usually reduces variance, but the larger neighborhood can blur local class structure and increase bias.\n",
    "\n",
    "Can we see this tradeoff in action by varying `K`? Let's test that on the dataset. Next let's consider our second design choice: the kernel width parameter $\\gamma$ in the RBF kernel.\n",
    "\n",
    "> __Gating parameter $\\gamma$:__\n",
    ">\n",
    "> The parameter $\\gamma$ controls the width of the RBF kernel and therefore the smoothness of the decision boundary. Smaller $\\gamma$ values produce smoother boundaries, while larger $\\gamma$ values produce more complex boundaries.\n",
    "\n",
    "Let's see how varying $\\gamma$ and the number of neighbors `K` affects the performance of our KNN classifier on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a00f7b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = let\n",
    "    \n",
    "    # initialize -\n",
    "    D = training; # specify what data set we are working with\n",
    "    number_of_training_examples = size(D,1); # how many examples are in the training dataset?\n",
    "    number_of_features = D[1].x |> length; # number of features in the dataset\n",
    "    Î³ = 0.50; # kernel parameter\n",
    "    m = 10; # neighborhood size multiple\n",
    "    C = 2; # number of classes\n",
    "\n",
    "    # fill up the feature matrix -\n",
    "    X = zeros(number_of_training_examples, number_of_features); # initialize a matrix to hold the features\n",
    "    for i âˆˆ 1:number_of_training_examples\n",
    "        X[i,:] = D[i].x; # fill the matrix with the features from the training dataset\n",
    "    end\n",
    "\n",
    "    # fill up the label vector -\n",
    "    y = zeros(number_of_training_examples); # initialize a vector to hold the labels\n",
    "    for i âˆˆ 1:number_of_training_examples\n",
    "        y[i] = D[i].y; # fill the vector with the labels from the training dataset\n",
    "    end\n",
    "\n",
    "    # build a model -\n",
    "    model = build(MyWeightedKernelizedKNNClassificationModel, (\n",
    "        K = (m*C+1), # we look at this many points\n",
    "        features = X,\n",
    "        labels = y,\n",
    "        k = (x,y) -> k(x,y,Î³), # RBF kernel similarity metric\n",
    "    ));\n",
    "\n",
    "    model; # return the model\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49874a88",
   "metadata": {},
   "source": [
    "### Inference\n",
    "Now that we have defined a kernel function, and built the model, let's use it to classify our data. We use the KNN classifier from [the `VLDataScienceMachineLearningPackage.jl` package](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl). \n",
    "\n",
    "> __What is going on in this code block?__\n",
    ">\n",
    "> In the code block below, we:\n",
    "> * Construct [a `MyWeightedKernelizedKNNClassificationModel` model](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/#VLDataScienceMachineLearningPackage.MyWeightedKernelizedKNNClassificationModel) using [a `build(...)` method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/factory/). The `model` instance holds the data for the problem, i.e., how many neighbors to look at `K`, and the kernel function $k$.\n",
    "> * Next, we pass this `model` instance to [the `classify(...)` method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/#VLDataScienceMachineLearningPackage.classify) which takes a test feature $\\mathbf{z}$ and the classifier `model` instance and returns the predicted label value $\\hat{y}$ for the test feature vector $\\mathbf{z}$.\n",
    "\n",
    "We return the predicted labels in `yÌ‚_KNN` and the actual labels in `y_KNN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22676098",
   "metadata": {},
   "outputs": [],
   "source": [
    "yÌ‚_KNN,y_KNN = let\n",
    "\n",
    "    # Data -\n",
    "    D = test; # what dataset are we working with?\n",
    "    number_of_test_examples = size(D,1);\n",
    "    number_of_features = D[1].x |> length; # number of features in the dataset\n",
    "\n",
    "     # fill up the feature matrix -\n",
    "    X = zeros(number_of_test_examples, number_of_features); # initialize a matrix to hold the features\n",
    "    for i âˆˆ 1:number_of_test_examples\n",
    "        X[i,:] = D[i].x; # fill the matrix with the features from the test dataset\n",
    "    end\n",
    "\n",
    "    # fill up the label vector (actual labels) -\n",
    "    y = zeros(number_of_test_examples); # initialize a vector to hold the labels\n",
    "    for i âˆˆ 1:number_of_test_examples\n",
    "        y[i] = D[i].y; # fill the vector with the labels from the test dataset\n",
    "    end\n",
    "\n",
    "    # process each vector in the test set, compare that to training (reference), and compute the predicted label -\n",
    "    yÌ‚ = zeros(number_of_test_examples);  # initialize some storage for the predicted label\n",
    "    for i âˆˆ 1:number_of_test_examples\n",
    "        z = X[i,:]; # get feature vector for test\n",
    "        Å·[i] = classify(z,model) # classify the test vector using the training data\n",
    "    end\n",
    " \n",
    "    # return -\n",
    "    Å·,y\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6159eb",
   "metadata": {},
   "source": [
    "### Performance\n",
    "We can evaluate the binary classifier's performance using various metrics. The central idea is to compare the predicted labels $\\hat{y}_{i}$ to the actual labels $y_{i}$ in the `test` dataset and measure wins (when the label is the same) and losses (label is different). This is easily represented in [the confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
    "\n",
    "> __Error analysis using the confusion matrix__\n",
    ">\n",
    "> Total mistakes (or mistake percentage) is only part of the story. We should understand whether we are biased toward false positives or false negatives. How many times did we predict a compound was biodegradable when it was not (false positive)? How many times did we predict not biodegradable when it was biodegradable (false negative)? For this we use a confusion matrix. \n",
    ">\n",
    "> The confusion matrix for a binary classifier is typically structured as:\n",
    ">\n",
    ">|                     | **Predicted Positive** | **Predicted Negative** |\n",
    ">|---------------------|------------------------|------------------------|\n",
    ">| **Actual Positive** | True Positive (TP)     | False Negative (FN)    |\n",
    ">| **Actual Negative** | False Positive (FP)    | True Negative (TN)     |\n",
    ">\n",
    "> From the confusion matrix, we derive three key metrics:\n",
    ">\n",
    "> * __Accuracy__ is the fraction of correct predictions overall: $\\texttt{accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$. It tells us the overall success rate but can be misleading if classes are imbalanced; a classifier that predicts \"negative\" for everything might achieve 95% accuracy on an imbalanced dataset.\n",
    ">\n",
    "> * __Precision__ answers: \"When we predict positive, how often are we right?\" $\\texttt{precision} = \\frac{TP}{TP + FP}$. In our biodegradation context, high precision means fewer false alarms, if you say a compound is biodegradable, it probably is.\n",
    ">\n",
    "> * __Recall__ (also called sensitivity) answers: \"Of all the true positives, how many did we catch?\" $\\texttt{recall} = \\frac{TP}{TP + FN}$. High recall means we're not missing degradable compounds. \n",
    "\n",
    "We compute the __confusion matrix__ to get these counts. We use the [confusion(...) method](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/binaryclassification/#VLDataScienceMachineLearningPackage.confusion), which takes actual labels and estimated labels, returning the confusion matrix. We save the confusion matrix in the `CM_KNN::Array{Int64,2}` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "566d588a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2Ã—2 Matrix{Int64}:\n",
       " 133  8\n",
       "  64  6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CM_KNN = confusion(y_KNN,yÌ‚_KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d9aeb1",
   "metadata": {},
   "source": [
    "Let's compute the __accuracy__, __precision__, and __recall__ for our KNN classifier using the confusion matrix. We save these metrics in the `accuracy_KNN::Float64`, `precision_KNN::Float64`, and `recall_KNN::Float64` variables, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4443d9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_KNN, precision_KNN, recall_KNN = let\n",
    "\n",
    "    # compute the confusion matrix -\n",
    "    CM = CM_KNN; # confusion matrix for KNN classifier\n",
    "\n",
    "    # compute accuracy, precision, and recall -\n",
    "    TP = CM[1,1]; # true positives\n",
    "    TN = CM[2,2]; # true negatives\n",
    "    FP = CM[2,1]; # false positives\n",
    "    FN = CM[1,2]; # false negatives\n",
    "\n",
    "    accuracy = (TP + TN) / sum(CM); # overall accuracy\n",
    "    precision = TP / (TP + FP); # precision\n",
    "    recall = TP / (TP + FN); # recall\n",
    "\n",
    "    (accuracy, precision, recall); # return the metrics\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8360a62e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6587677725118484, 0.6751269035532995, 0.9432624113475178)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_KNN, precision_KNN, recall_KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec5d1cd",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b96cc5",
   "metadata": {},
   "source": [
    "## Task 3: Let's do a hyperparameter sweep\n",
    "In this task, we perform a grid search to see how the number of neighbors `K` and kernel gating parameter $\\gamma$ affect classification performance on the test dataset.\n",
    "\n",
    "> __What is going on in this code block?__\n",
    ">\n",
    "> We sweep over several $\\left(K,\\gamma\\right)$ combinations. For each pair, we build a `MyWeightedKernelizedKNNClassificationModel`, classify every test point, compute a confusion matrix, then calculate accuracy, precision, and recall. Finally, we sort all results (by the overall accuracy) so the best settings appear first.\n",
    "\n",
    "Let's execute the sweep and inspect the top-performing settings. We store the results in the `knn_sweep_results::DataFrame` variable, which contains the `K`, `gamma`, `TP`, `TN`, `FP`, `FN`, `accuracy`, `precision`, and `recall` values for each combination. We sort this DataFrame by accuracy in descending order to see the best-performing hyperparameter settings at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ef59ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_sweep_results = let\n",
    "\n",
    "    # load data -\n",
    "    Dtr = training;\n",
    "    Dte = test;\n",
    "    ntr = length(Dtr);\n",
    "    nte = length(Dte);\n",
    "    nfeatures = length(Dtr[1].x);\n",
    "\n",
    "    # feature/label arrays for training and test -\n",
    "    Xtr = zeros(ntr, nfeatures);\n",
    "    ytr = zeros(ntr);\n",
    "    for i in 1:ntr\n",
    "        Xtr[i, :] = Dtr[i].x;\n",
    "        ytr[i] = Dtr[i].y;\n",
    "    end\n",
    "\n",
    "    Xte = zeros(nte, nfeatures);\n",
    "    yte = zeros(nte);\n",
    "    for i in 1:nte\n",
    "        Xte[i, :] = Dte[i].x;\n",
    "        yte[i] = Dte[i].y;\n",
    "    end\n",
    "\n",
    "    # hyperparameter grids -\n",
    "    K_grid = [1, 3, 5, 9, 15, 21, 31, 41, 51, 71, 101, 151, 201, 301, 401, 501, 701, 843];\n",
    "    Î³_grid = [0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 1.00];\n",
    "\n",
    "    results = DataFrame(\n",
    "        K = Int[],\n",
    "        gamma = Float64[],\n",
    "        TP = Int[],\n",
    "        TN = Int[],\n",
    "        FP = Int[],\n",
    "        FN = Int[],\n",
    "        accuracy = Float64[],\n",
    "        precision = Float64[],\n",
    "        recall = Float64[]\n",
    "    );\n",
    "\n",
    "    # sweep all (K, gamma) combinations -\n",
    "    for Î³ in Î³_grid\n",
    "        for K_neighbors in K_grid\n",
    "            model_local = build(MyWeightedKernelizedKNNClassificationModel, (\n",
    "                K = K_neighbors,\n",
    "                features = Xtr,\n",
    "                labels = ytr,\n",
    "                k = (x, y) -> k(x, y, Î³),\n",
    "            ));\n",
    "\n",
    "            # classify test set -\n",
    "            yÌ‚ = zeros(nte);\n",
    "            for i in 1:nte\n",
    "                yÌ‚[i] = classify(Xte[i, :], model_local);\n",
    "            end\n",
    "\n",
    "            # confusion-matrix metrics -\n",
    "            CM = confusion(yte, yÌ‚);\n",
    "            TP = CM[1, 1];\n",
    "            TN = CM[2, 2];\n",
    "            FP = CM[2, 1];\n",
    "            FN = CM[1, 2];\n",
    "\n",
    "            accuracy = (TP + TN) / sum(CM);\n",
    "            precision = (TP + FP) == 0 ? 0.0 : TP / (TP + FP);\n",
    "            recall = (TP + FN) == 0 ? 0.0 : TP / (TP + FN);\n",
    "\n",
    "            push!(results, (\n",
    "                K = K_neighbors,\n",
    "                gamma = Î³,\n",
    "                TP = TP,\n",
    "                TN = TN,\n",
    "                FP = FP,\n",
    "                FN = FN,\n",
    "                accuracy = accuracy,\n",
    "                precision = precision,\n",
    "                recall = recall\n",
    "            ));\n",
    "        end\n",
    "    end\n",
    "\n",
    "    df = sort(results, :accuracy, rev = true); # sort results by accuracy (descending order);\n",
    "    df[1:10, :] # return the top 10 results\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a130ac18",
   "metadata": {},
   "source": [
    "Next, let's build a table of the results from our hyperparameter search using [the `pretty_table(...)` function exported by the `PrettyTables.jl` package](https://github.com/ronisbr/PrettyTables.jl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fe41dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------- --------- ------- ------- ------- ------- ---------- ----------- ----------\n",
      " \u001b[1m     K \u001b[0m \u001b[1m   gamma \u001b[0m \u001b[1m    TP \u001b[0m \u001b[1m    TN \u001b[0m \u001b[1m    FP \u001b[0m \u001b[1m    FN \u001b[0m \u001b[1m accuracy \u001b[0m \u001b[1m precision \u001b[0m \u001b[1m   recall \u001b[0m\n",
      " \u001b[90m Int64 \u001b[0m \u001b[90m Float64 \u001b[0m \u001b[90m Int64 \u001b[0m \u001b[90m Int64 \u001b[0m \u001b[90m Int64 \u001b[0m \u001b[90m Int64 \u001b[0m \u001b[90m  Float64 \u001b[0m \u001b[90m   Float64 \u001b[0m \u001b[90m  Float64 \u001b[0m\n",
      " ------- --------- ------- ------- ------- ------- ---------- ----------- ----------\n",
      "    843       0.1     117      55      15      24   0.815166    0.886364   0.829787\n",
      "    843      0.25     119      53      17      22   0.815166       0.875   0.843972\n",
      "    843      0.75     120      51      19      21   0.810427    0.863309   0.851064\n",
      "    843       0.5     120      50      20      21   0.805687    0.857143   0.851064\n",
      "    843       1.0     120      50      20      21   0.805687    0.857143   0.851064\n",
      "    843      0.05     115      54      16      26   0.800948    0.877863   0.815603\n",
      "    843      0.01     129      35      35      12   0.777251    0.786585   0.914894\n",
      "     31       0.5     137       7      63       4   0.682464       0.685   0.971631\n",
      "     31       0.1     136       7      63       5   0.677725    0.683417   0.964539\n",
      "      9      0.01     141       1      69       0   0.672986    0.671429        1.0\n",
      " ------- --------- ------- ------- ------- ------- ---------- ----------- ----------\n"
     ]
    }
   ],
   "source": [
    "pretty_table(knn_sweep_results; \n",
    "    backend = :text,\n",
    "    fit_table_in_display_horizontally = false,\n",
    "    table_format = TextTableFormat(borders = text_table_borders__compact)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f900d678",
   "metadata": {},
   "source": [
    "__Question:__ Which $(K, \\gamma)$ pair appears at the top of `knn_sweep_results`? How does the best configuration's accuracy/recall compare to the baseline configuration from Task 2? If your boss asked you to pick a single configuration for the KNN classifier, which one would you pick and why?\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aeb38a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This notebook builds a weighted kernelized KNN classifier for QSAR biodegradation data and evaluates how neighborhood and kernel choices affect classification quality.\n",
    "\n",
    "> __Key Takeaways:__\n",
    ">\n",
    "> * **Data preparation controls compatibility:** Converting rows into feature-label tuples and using binary target encoding creates the model-ready data structure. This preprocessing step ensures the classifier receives consistent inputs across training and testing.\n",
    "> * **Kernel checks support model setup:** The RBF kernel passes a positive-semidefinite check on the training kernel matrix in this notebook. This supports using the kernelized similarity calculation for the KNN workflow demonstrated here.\n",
    "> * **Hyperparameters change performance tradeoffs:** Changing neighbor count and kernel width changes the accuracy, precision, and recall tradeoffs. The sweep results provide a concrete basis for choosing settings before cross-validation.\n",
    "\n",
    "Next step: use cross-validation to select neighbor count and kernel width before final model reporting.\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12.5",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
